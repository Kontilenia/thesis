{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148f572b-3735-440f-8c00-d7fc670bb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import langgraph\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from typing import TypedDict, Dict, List\n",
    "import requests\n",
    "from langgraph.graph import StateGraph, END, START, StateGraph\n",
    "import logging\n",
    "from typing import TypedDict, Dict, List\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38258ab1-07ad-4360-aa65-ab84c2e22f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph\n",
    "!pip install boto3\n",
    "!pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbf3714-4ff8-4801-9855-748f99882645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    aws_access_key_id = secrets.get('AWS_ACCESS_KEY')\n",
    "    aws_secret_access_key = secrets.get('AWS_SECRET_ACCESS_KEY')\n",
    "    wiki_key = secrets.get('WIKIPEDIA_KEY')\n",
    "\n",
    "# Create a session using your AWS credentials\n",
    "session = boto3.Session(aws_access_key_id=aws_access_key_id,\n",
    "                        aws_secret_access_key=aws_secret_access_key,\n",
    "                        region_name='us-west-2'\n",
    ")\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock_client = session.client('bedrock-runtime')\n",
    "model_id = 'anthropic.claude-3-opus-20240229-v1:0'\n",
    "\n",
    "\n",
    "def call_claude(messages: list, model_id: str) -> str:\n",
    "    body = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 300,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body['content'][0]['text']\n",
    "\n",
    "def get_wikipedia_intro(title: str) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Fetch the introductory paragraph(s) from a Wikipedia page for a given title.\n",
    "\n",
    "    Args:\n",
    "        title (str): The Wikipedia page title to query.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing:\n",
    "            - 'information': List[str] of extracted intro text lines.\n",
    "            - 'metadata': str indicating status ('one entry', 'multiple entries',\n",
    "                          'no info', or 'respond error').\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {'Authorization': wiki_key, 'User-Agent': 'thesis'}\n",
    "    endpoint = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'action': 'query',\n",
    "        'prop': 'extracts|pageprops',\n",
    "        'exintro': 1,\n",
    "        'explaintext': 1,\n",
    "        'redirects': 1,\n",
    "        'titles': title,\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, params=params, headers=headers, timeout=10)\n",
    "\n",
    "    if not response.ok:\n",
    "        return {'information': [], 'metadata': 'respond error'}\n",
    "\n",
    "    data = response.json()\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    extract = page.get('extract', '')\n",
    "    pageprops = page.get('pageprops', [])\n",
    "    \n",
    "    is_disambiguation = any('disambiguation' in str(cat).lower() for cat in pageprops)\n",
    "\n",
    "    if not extract:\n",
    "        return {'information': [], 'metadata': 'no info'}\n",
    "\n",
    "    # Happy path return â€” only set what's needed\n",
    "    if is_disambiguation:\n",
    "        metadata = 'multiple entries'\n",
    "        intro_list = extract\n",
    "    else:\n",
    "        intro_list = get_wikipedia_summary(title)\n",
    "        metadata = 'one entry'\n",
    "\n",
    "    return {'introduction':intro_list, 'information': extract, 'metadata': metadata}\n",
    "\n",
    "\n",
    "def get_wikipedia_summary(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch a short summary for a Wikipedia page.\n",
    "\n",
    "    Args:\n",
    "        title (str): The Wikipedia page title.\n",
    "\n",
    "    Returns:\n",
    "        str: The short summary text, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    # Replace spaces with underscores for the URL\n",
    "    title = title.replace(\" \", \"_\")\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"thesis\"}, timeout=10)\n",
    "\n",
    "    if not response.ok:\n",
    "        return \"\"\n",
    "\n",
    "    data = response.json()\n",
    "    return data.get(\"extract\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adde2b3-3bca-4e52-b99a-d0d40c3f6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          +-----------+              \n",
      "                          | __start__ |              \n",
      "                          +-----------+              \n",
      "                                 *                   \n",
      "                                 *                   \n",
      "                                 *                   \n",
      "                       +------------------+          \n",
      "                       | detect_name_node |          \n",
      "                       +------------------+          \n",
      "                       ...               ....        \n",
      "                    ...                      ...     \n",
      "                  ..                            ...  \n",
      "+--------------------------+                       ..\n",
      "| get_wikipedia_intro_node |                        .\n",
      "+--------------------------+                        .\n",
      "              *                                     .\n",
      "              *                                     .\n",
      "              *                                     .\n",
      "+---------------------------+                       .\n",
      "| check_info_relevancy_node |                       .\n",
      "+---------------------------+                       .\n",
      "              *                                     .\n",
      "              *                                     .\n",
      "              *                                     .\n",
      "   +---------------------+                         ..\n",
      "   | tailor_summary_node |                      ...  \n",
      "   +---------------------+                   ...     \n",
      "                       ***               ....        \n",
      "                          ***         ...            \n",
      "                             **     ..               \n",
      "                            +---------+              \n",
      "                            | __end__ |              \n",
      "                            +---------+              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 19:04:54,449 - INFO - No name detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I wish I was an Asian guy', 'name': 'John', 'contains_name': False}\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# State\n",
    "class AgentState(TypedDict):\n",
    "    text: str\n",
    "    name: str\n",
    "    contains_name: bool\n",
    "    wiki_search: Dict[str, List]\n",
    "    name_mapping: bool\n",
    "    tailored_summary: str\n",
    "\n",
    "# Nodes\n",
    "def detect_name_node(state: AgentState) -> AgentState:\n",
    "    sentence = state[\"text\"]\n",
    "    entity = state[\"name\"]\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"\"\"Does entity '{entity}' in the following text \n",
    "                        refer to a person's name? Respond only with true or false.\n",
    "                        '{sentence}'\"\"\"}\n",
    "    ]\n",
    "    result = call_claude(messages, model_id).strip().lower()\n",
    "    state[\"contains_name\"] = result == \"true\"\n",
    "    if state[\"contains_name\"]:\n",
    "        logger.info(\"Name detected\")\n",
    "    else:\n",
    "        logger.info(\"No name detected\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def get_wikipedia_intro_node(state: AgentState) -> AgentState:\n",
    "    state[\"wiki_search\"] = get_wikipedia_intro(state[\"name\"])\n",
    "    logging.info(\"Information retrieved\")\n",
    "    return state\n",
    "\n",
    "\n",
    "# Conditional routing\n",
    "def route_after_detect(state: AgentState) -> str:\n",
    "    return \"has_name\" if state[\"contains_name\"] else \"no_name\"\n",
    "\n",
    "\n",
    "def check_info_relevancy_node(state: AgentState) -> AgentState:\n",
    "    sentence = state[\"text\"]\n",
    "    wiki_info = \" \".join(state[\"wiki_search\"]['introduction'])\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"\"\"Is the following Wikipedia info relevant to the entity '{state['name']}' \n",
    "                        in the context of this text? Respond only with true or false.\n",
    "                        TEXT: {sentence}\n",
    "                        WIKI INFO: {wiki_info}\"\"\"}\n",
    "    ]\n",
    "    result = call_claude(messages, model_id).strip().lower()\n",
    "    state[\"name_mapping\"] = result == \"true\"\n",
    "    if state[\"name_mapping\"]:\n",
    "        logger.info(\"Name matched with retrieved info\")\n",
    "    else:\n",
    "        logger.info(\"Name didn't match with retrieved info\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def tailor_summary_node(state: AgentState) -> AgentState:\n",
    "    sentence = state[\"text\"]\n",
    "    wiki_info = \" \".join(state[\"wiki_search\"]['information'])\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"\"\"You are given:\n",
    "            1. A sentence from a text.\n",
    "            2. Wikipedia information about the entity '{state['name']}'.\n",
    "            Your task:\n",
    "            - Assume the reader (another LLM) does not know who this entity is.\n",
    "            - From the Wikipedia info, select only the facts needed to:\n",
    "              a) Identify who the entity is.\n",
    "              b) Mention relevant information that could be used as a context \n",
    "              for the sentence.\n",
    "            - Ignore unrelated details that do not help interpret the sentence.\n",
    "            - Combine these into a single, concise, self-contained summary that\n",
    "            would let the reader fully understand the reference in the sentence.\n",
    "\n",
    "            TEXT: {sentence}\n",
    "            WIKIPEDIA INFO: {wiki_info}\n",
    "\n",
    "            Return only the summary, no extra commentary.\n",
    "            \"\"\"}\n",
    "    ]\n",
    "    result = call_claude(messages, model_id).strip()\n",
    "    state[\"tailored_summary\"] = result\n",
    "    logger.info(\"Tailored information to text.\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Graph build\n",
    "myGraph = StateGraph(AgentState)\n",
    "\n",
    "myGraph.add_node(\"detect_name_node\", detect_name_node)\n",
    "myGraph.add_node(\"get_wikipedia_intro_node\", get_wikipedia_intro_node)\n",
    "myGraph.add_node(\"check_info_relevancy_node\", check_info_relevancy_node)\n",
    "myGraph.add_node(\"tailor_summary_node\", tailor_summary_node)\n",
    "\n",
    "myGraph.add_edge(START, \"detect_name_node\")\n",
    "\n",
    "# Conditional edges with explicit mapping\n",
    "myGraph.add_conditional_edges(\n",
    "    \"detect_name_node\",\n",
    "    route_after_detect,\n",
    "    {\n",
    "        \"has_name\": \"get_wikipedia_intro_node\",\n",
    "        \"no_name\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "myGraph.add_edge(\"get_wikipedia_intro_node\", \"check_info_relevancy_node\")\n",
    "myGraph.add_edge(\"check_info_relevancy_node\", \"tailor_summary_node\")\n",
    "myGraph.add_edge(\"tailor_summary_node\", END)\n",
    "\n",
    "compiled_graph = myGraph.compile()\n",
    "print(compiled_graph.get_graph().draw_ascii())\n",
    "\n",
    "result = compiled_graph.invoke({\"name\": \"John\",\n",
    "                     \"text\": \"I wish I was an Asian guy\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55b158b3-92ee-4553-a5dd-9dec627d2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = compiled_graph.get_graph().draw_mermaid_png()\n",
    "with open(\"information_extraction_graph.png\", mode=\"wb\") as f:\n",
    "    f.write(image_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
