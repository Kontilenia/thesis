{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7fa1b3-4729-4ef9-bb48-af5fe50575ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from peft import PeftModel\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0e0dc-bb03-41eb-920b-0706f0204810",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install peft\n",
    "!pip install transformers\n",
    "!pip install einops\n",
    "!pip install sentencepiece\n",
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a578c0-a2de-478d-8b6d-ec862e255d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/ec2-user/SageMaker\"\n",
    "os.environ['HF_HOME'] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f064e68-e8f5-4c08-95fe-39623332c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_names(df: pd.DataFrame,\n",
    "                    new_column_name:\n",
    "                    str = \"recognized_names\"):\n",
    "\n",
    "    base_model_path = \"internlm/internlm2_5-7b\"\n",
    "    lora_path = \"Umean/B2NER-Internlm2.5-7B-LoRA\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path,\n",
    "                                              trust_remote_code=True,\n",
    "                                              cache_dir=cache_dir)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model, lora_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    def extract_names(text, chunk_size=1000):\n",
    "        # Split text into manageable chunks\n",
    "        def chunk_text(text, size):\n",
    "            return [text[i:i+size] for i in range(0, len(text), size)]\n",
    "\n",
    "        chunks = chunk_text(text, chunk_size)\n",
    "        all_names = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            prompt = (\"Recognize all people names in the following text. \"\n",
    "                      \"Format the answer as: person name: entity; person name: entity. \\n\\n\"\n",
    "                      f\"Text: {chunk} \\nAnswer:\")\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                [prompt],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                # truncation=True  # ensure the prompt fits the model\n",
    "            ).to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    use_cache=False\n",
    "                )\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            response = generated_text.split(\"Answer:\")[-1].strip()\n",
    "            if \"None\" not in generated_text:\n",
    "                all_names.append(response)\n",
    "\n",
    "        return \"; \".join(all_names).strip()\n",
    "\n",
    "    unique_pairs = df[[\"interview_question\", \"interview_answer\"]].drop_duplicates()\n",
    "\n",
    "    names_map = {\n",
    "        row[\"interview_question\"]: extract_names(str(row['interview_question'] + row['interview_answer']))\n",
    "        for _, row in unique_pairs.iterrows()\n",
    "    }\n",
    "\n",
    "    df[new_column_name] = df[\"interview_question\"].map(names_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv('preprocessed_data/train_set.csv')\n",
    "recognise_names(df,\n",
    "                \"recognised_names\")\n",
    "\n",
    "file_path = \"./preprocessed_data/named_train_set.csv\"\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6fecf-a062-4b67-ac0f-36247ee728be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        size (int): The maximum size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list containing text chunks.\n",
    "    \"\"\"\n",
    "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
    "\n",
    "\n",
    "def extract_names(text: str,\n",
    "                  tokenizer: AutoTokenizer,\n",
    "                  model: torch.nn.Module,\n",
    "                  chunk_size: int = 1000) -> str:\n",
    "    \"\"\"\n",
    "    Extracts person names from the input text using a language model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing potential person names.\n",
    "        tokenizer (AutoTokenizer): Tokenizer associated with the language\n",
    "        model.\n",
    "        model (torch.nn.Module): The fine-tuned language model used for name\n",
    "        extraction.\n",
    "        chunk_size (int, optional): Maximum characters per chunk for processing.\n",
    "        Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated string of recognized person names and entities.\n",
    "    \"\"\"\n",
    "    chunks = chunk_text(text, chunk_size)\n",
    "    all_names = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            \"Recognize all people names in the following text. \"\n",
    "            \"Format the answer as: person name: entity; person name: entity. \\n\\n\"\n",
    "            f\"Text: {chunk} \\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            [prompt],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                use_cache=False\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"Answer:\")[-1].strip()\n",
    "        if \"None\" not in generated_text:\n",
    "            all_names.append(response)\n",
    "\n",
    "    return \"; \".join(all_names).strip()\n",
    "\n",
    "\n",
    "def recognise_names(df: pd.DataFrame,\n",
    "                    new_column_name: str = \"recognized_names\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recognizes person names from interview questions and answers using a fine-tuned LLM.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'interview_question' and 'interview_answer'.\n",
    "        new_column_name (str, optional): Name of the new column to store recognized names. Defaults to 'recognized_names'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional column of recognized names.\n",
    "    \"\"\"\n",
    "    base_model_path = \"internlm/internlm2_5-7b\"\n",
    "    lora_path = \"Umean/B2NER-Internlm2.5-7B-LoRA\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path,\n",
    "                                              trust_remote_code=True,\n",
    "                                              cache_dir=cache_dir)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model, lora_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    unique_pairs = df[[\"interview_question\", \"interview_answer\"]].drop_duplicates()\n",
    "\n",
    "    names_map = {\n",
    "        row[\"interview_question\"]: extract_names(\n",
    "            str(row[\"interview_question\"] + row[\"interview_answer\"]),\n",
    "            tokenizer,\n",
    "            model\n",
    "        )\n",
    "        for _, row in unique_pairs.iterrows()\n",
    "    }\n",
    "\n",
    "    df[new_column_name] = df[\"interview_question\"].map(names_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c99fef-3c94-4744-af98-8a7672990eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('preprocessed_data/train_set.csv')\n",
    "recognise_names(train_df, \"recognised_names\")\n",
    "\n",
    "file_path = \"./preprocessed_data/named_train_set.csv\"\n",
    "train_df.to_csv(file_path)\n",
    "\n",
    "test_df = pd.read_csv('preprocessed_data/test_set.csv')\n",
    "recognise_names(test_df, \"recognised_names\")\n",
    "\n",
    "file_path = \"./preprocessed_data/named_test_set.csv\"\n",
    "test_df.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
