{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54152fe-3dbf-4edf-a2b5-d78a382837c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install -U bitsandbytes\n",
    "!pip install huggingface-hub\n",
    "!pip install datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9043e0b-2efc-4a9f-b06a-719178cf1352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          pipeline,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          PreTrainedTokenizer)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import huggingface_hub\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix)\n",
    "from typing import List\n",
    "import wandb\n",
    "from lora_llm import finetuning, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba924e95-2e70-46a6-bd47-540427028a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find lora_llm.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkontilenia\u001b[0m (\u001b[33mkontilenia-national-technical-university-of-athens\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/thesis/wandb/run-20250921_101756-rp8nebnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity/runs/rp8nebnt' target=\"_blank\">7 epoch Ministral</a></strong> to <a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity' target=\"_blank\">https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity/runs/rp8nebnt' target=\"_blank\">https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity/runs/rp8nebnt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be33b433ab46478294e21bc4b34f698c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7667712 || all params: 4554600448\n",
      "        || trainable%: 0.16835092534553758\n",
      "Found 3109 instances for training and\n",
      "    339 instances for validation.\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  62/2720 08:46 < 6:28:45, 0.11 it/s, Epoch 0.16/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'lora_llm'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "\n",
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    huggingface_hub.login(secrets.get('HF_KEY'))\n",
    "    wandb.login(key=secrets.get('WANDB_KEY'))\n",
    "\n",
    "lr = 2e-4\n",
    "epochs = 7\n",
    "class_names = []\n",
    "base_model_name = 'mistralai/Ministral-8B-Instruct-2410'\n",
    "label_name = \"evasion_label\"\n",
    "fine_tuned_model_path = f\"./ministral_{epochs}ep\"\n",
    "\n",
    "# Wandb configuration\n",
    "run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "                 project='political-speech-clarity',\n",
    "                 job_type=\"training\",\n",
    "                 name=str(epochs)+\" epoch Ministral\",\n",
    "                 # Track hyperparameters and run metadata\n",
    "                 config={\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"architecture\": base_model_name,\n",
    "                    \"dataset\": \"qevasion_dataset_preproccessed\",\n",
    "                    \"epochs\": epochs,\n",
    "                 })\n",
    "\n",
    "model, tokenizer = finetuning(base_model_name,\n",
    "                              fine_tuned_model_path,\n",
    "                              label_name,\n",
    "                              lr,\n",
    "                              epochs)\n",
    "\n",
    "evaluate(base_model_name,\n",
    "         fine_tuned_model_path,\n",
    "         \"evasion_label\",\n",
    "         \"clarity_label\",\n",
    "         \"preprocessed_data/test_set_unnamed.csv\",\n",
    "         False,\n",
    "         model,\n",
    "         tokenizer,\n",
    "         run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a80e73-d400-4cde-8702-6f93c6a194a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
