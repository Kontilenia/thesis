{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m155.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.1 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.0\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (2.2.2)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (4.48.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.27.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.3.0 peft-0.14.0\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.0\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub) (2024.8.30)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->datasets) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Installing collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.17\n",
      "    Uninstalling multiprocess-0.70.17:\n",
      "      Successfully uninstalled multiprocess-0.70.17\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install -U bitsandbytes\n",
    "!pip install huggingface-hub\n",
    "!pip install datasets\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JF6TQ3ctdtsJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import argparse\n",
    "import huggingface_hub\n",
    "import os\n",
    "import logging\n",
    "# import wandb\n",
    "\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "\n",
    "def create_prompted_text(\n",
    "    dataset,\n",
    "    label_name,\n",
    "):\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel: {row[label_name]}\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def create_test_prompted_text(\n",
    "    dataset,\n",
    "    label_name\n",
    "):\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel: \"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_qevasion_dataset(\n",
    "    tokenizer,\n",
    "    label_name=\"clarity_label\"\n",
    "):\n",
    "    # Get train set data\n",
    "    df = pd.read_csv('preprocessed_data/train_set.csv')[['question',\n",
    "                                                         'interview_question',\n",
    "                                                         'interview_answer',\n",
    "                                                         label_name]]\n",
    "\n",
    "    # Split train set to train and validation data\n",
    "    np.random.seed(2024)\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    train = df[msk]\n",
    "    validation = df[~msk]\n",
    "\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_texts = create_prompted_text(train, label_name)\n",
    "    validation_texts = create_prompted_text(validation,\n",
    "                                            label_name)\n",
    "\n",
    "    # print(\"Example of train test:\" + train_texts[1])\n",
    "    # print(\"Example of validation test:\" + validation_texts[1])\n",
    "\n",
    "    train_texts = train_texts[:10]\n",
    "    validation_texts = validation_texts[:1]\n",
    "    return (CustomTextDataset(train_texts, tokenizer),\n",
    "            CustomTextDataset(validation_texts, tokenizer))\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"\"\"trainable params: {trainable_params} || all params: {all_param}\n",
    "        || trainable%: {100 * trainable_params / all_param}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def main(model_name,\n",
    "         output_model_dir,\n",
    "         label_taxonomy):\n",
    "\n",
    "    # TO-DO: Make wandb functional\n",
    "    # wandb.init(\n",
    "    #         project='Prediction of clarity on political speech',\n",
    "    #         job_type=\"training\",\n",
    "\n",
    "    #         # track hyperparameters and run metadata\n",
    "    #         config={\n",
    "    #             \"learning_rate\": 0.02,\n",
    "    #             \"architecture\": \"CNN\",\n",
    "    #             \"dataset\": \"CIFAR-100\",\n",
    "    #             \"epochs\": 10,\n",
    "    #         }\n",
    "    #       )\n",
    "\n",
    "    cache_dir = \"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        # load_in_4bit=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        # device_map='auto',\n",
    "        device_map='cpu',\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True,)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "    # Reduce number of stored activation\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,  # Attention heads\n",
    "        lora_alpha=32,  # Alpha scaling\n",
    "        # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"  # set this for CLM or Seq2Seq\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # load data\n",
    "    train_data, validation_data = load_qevasion_dataset(tokenizer,\n",
    "                                                        label_taxonomy)\n",
    "\n",
    "    print(f\"\"\"Found {len(train_data)} instances for training and\n",
    "    {len(validation_data) } instances for validation.\"\"\")\n",
    "\n",
    "    epochs = 1\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training...\")\n",
    "    out_dir = output_model_dir.split(\"/\")[-1]\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=validation_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=8,\n",
    "            warmup_steps=100,\n",
    "            max_steps=int(len(train_data)*epochs/8),\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            # logging_steps=10,\n",
    "            eval_steps=0.2,\n",
    "            eval_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            output_dir=f'outputs_{out_dir}'\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                                   mlm=False)\n",
    "    )\n",
    "\n",
    "    # TO-DO: Make wandb functional\n",
    "    # wandb.finish()\n",
    "\n",
    "    # silence the warnings. Re-enable for inference!\n",
    "    model.config.use_cache = False\n",
    "    trainer.train()\n",
    "    # Save the model\n",
    "    model.save_pretrained(output_model_dir)\n",
    "\n",
    "    # Optionally, save the tokenizer as well\n",
    "    tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-ZUoMXBrMnw",
    "outputId": "599f1741-9849-4425-9325-139f7d4d0778"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099565cf88194ab0bf76ef4f3cc567b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "huggingface_hub.login(os.environ[\"hf_key\"])\n",
    "# wandb.login(os.environ[\"hf_key\"])\n",
    "\n",
    "model = main('meta-llama/Llama-3.1-8B-Instruct',\n",
    "             output_model_dir=\"./data\",\n",
    "             label_taxonomy=\"clarity_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------\n",
    "                                    **Model evaluation before fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18033f7d6f354308ab331ecbc2a986fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3af970cbd2f401e8f0659623b9b1b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc3094dc6774428b1d02784fab39897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321ea23c182a4b15a8f7ee6a3787a07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e04bb15cc143cd934c22cb54d743ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51e6294d3d41bebbb05b33b12279c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29576a933a834fb7b63d168a45e74fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a51c96c8b412fb69d769b034bcd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ac656610154ab396510aaa0733c645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c607ca0470b74fd48f7e29c093568d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b767a8e7f0475fb1db3ba2ba73f475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0848675fd7754ddd9200df4fb61f40d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "huggingface_hub.login(os.environ[\"hf_key\"])\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map='cpu',\n",
    "    # device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  1%|          | 7/680 [00:27<35:04,  3.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 30/680 [01:44<31:25,  2.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      "\n",
      "### part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 38/680 [02:01<23:36,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 55/680 [02:43<22:36,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 70/680 [03:31<32:23,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 75/680 [03:42<24:38,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 77/680 [03:47<22:55,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 81/680 [03:54<16:37,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 87/680 [04:03<16:20,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 88/680 [04:04<14:50,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 99/680 [04:28<17:11,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 121/680 [05:18<17:52,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 126/680 [05:29<17:36,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 131/680 [05:38<16:47,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 169/680 [07:06<16:22,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 173/680 [07:17<19:50,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 179/680 [07:33<23:29,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 180/680 [07:35<20:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 184/680 [07:43<17:17,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 191/680 [08:00<17:22,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 210/680 [09:00<23:10,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 226/680 [09:33<12:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 238/680 [10:04<15:34,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 282/680 [11:28<09:30,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 365/680 [15:48<17:44,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 408/680 [19:53<16:10,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 417/680 [20:30<13:04,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 418/680 [20:32<10:39,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 430/680 [21:11<09:38,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 467/680 [23:34<10:31,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 469/680 [23:40<09:30,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 479/680 [24:17<11:22,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 534/680 [27:50<06:12,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 540/680 [28:05<05:23,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 580/680 [30:18<05:58,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 628/680 [32:57<03:23,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 673/680 [35:37<00:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [35:57<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import (AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          pipeline)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "label_name = \"clarity_label\"\n",
    "\n",
    "# Get train set data\n",
    "df = pd.read_csv('preprocessed_data/train_set.csv')[['question',\n",
    "                                                     'interview_question',\n",
    "                                                     'interview_answer',\n",
    "                                                     label_name]]\n",
    "\n",
    "# Split train set to train and validation data\n",
    "np.random.seed(2024)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "validation = df[~msk]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_texts = create_prompted_text(train, label_name)\n",
    "validation_texts = create_prompted_text(validation,\n",
    "                                        label_name)\n",
    "\n",
    "# print(\"Example of train test:\" + train_texts[1])\n",
    "# print(\"Example of validation test:\" + validation_texts[1])\n",
    "\n",
    "\n",
    "def predict(test, model, tokenizer):\n",
    "\n",
    "    # Set logging level to ERROR to suppress INFO messages\n",
    "    logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "    y_pred = []\n",
    "    categories = list(df[label_name].unique())\n",
    "    pipe = pipeline(task=\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=4,\n",
    "                    temperature=0.1\n",
    "                    )\n",
    "\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "\n",
    "        # Determine the predicted category\n",
    "        for category in categories:\n",
    "            if category.lower() in answer.lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            print(\"Wrong label:\" + answer.lower())\n",
    "            y_pred.append(\"none\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "validation_texts = create_prompted_text(validation, label_name)\n",
    "\n",
    "dataset = pd.DataFrame(validation_texts, columns=['text'])\n",
    "y_pred = predict(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.341\n",
      "Accuracy for label Direct Reply: 0.853\n",
      "Accuracy for label Indirect: 0.013\n",
      "Accuracy for label Direct Non-Reply: 0.562\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Direct Reply       0.38      0.85      0.53       218\n",
      "        Indirect       0.33      0.01      0.02       389\n",
      "Direct Non-Reply       0.30      0.56      0.39        73\n",
      "\n",
      "       micro avg       0.36      0.34      0.35       680\n",
      "       macro avg       0.34      0.48      0.31       680\n",
      "    weighted avg       0.34      0.34      0.22       680\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[186   2  20]\n",
      " [284   5  77]\n",
      " [ 20   8  41]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = list(df[label_name].unique())\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "y_true = validation[label_name]\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'none',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classify the interviewee's response into one of the following\\n        categories: Indirect, Direct Reply, Direct Non-Reply. Analyze the part of the interview provided below, focusing\\n        specifically on the interviewee's response to the marked question.\\n\\n ### Part of the interview ### \\n Intervier:Q. Hi, I'm Jongjin Park of Money Today. First of all, I would like to ask a question to President Yoon Suk Yeol of the Republic of Korea. I heard that you stated that a new chapter has opened in our trilateral cooperation with the two countries. Compared to the previous summits, what would be the most significant outcome that you gained through this summit?And also, from the perspective of our people, what would be the benefit that the people of Korea would feel from these strengthening of ties?And now my question goes to President Biden. During this summit, the issues of detainees or prisoners of wars—and you mentioned that there will be further cooperation in these human rights issues. And you also said you will support the free and peaceful Korean Peninsula in the region. And what would—what kind of shift would there be in your policy? And what kind of specific solutions do you have in this regard?Lastly, I would like to direct my question to Prime Minister Kishida. Today we had a historic trilateral summit. However, there was much backlash and many concerns in Korea. However, President Yoon showed his political courage to do so. That's the international community's evaluation.However, there are still concerns that the—Japan is making very passive efforts to resolve our issues that still remain. And also, how would you be able to show your truthful willingness to resolve and improve our bilateral relations going forward? \\n Response: First of all, this trilateral cooperation amongst our three countries has opened a new chapter, and we made that announcement today to talk about the differences from the past cooperation.For instance, in the past, it was about individual issues that we sought cooperation among ourselves. But now, as we have opened a new chapter in our cooperation for security, economy, science and technology, and development cooperation for the Global South, health, and women—across all of these issues, our three countries decided to closely work together. So it's much more comprehensive in nature.Such comprehensive cooperation has been launched by us today because currently we face complicated crisis and the threat from the D.P.R.K. And across the world, we believe that we can together make a contribution to freedom and peace around the world.So that is our foundational understanding and our common and shared interests of the three countries. And not just for exclusionary interests of ourselves. Our interests are well aligned with the universal interests of the members of the global community. That's where we find our shared interests lie.And at the same time, this framework of comprehensive cooperation among our three countries will contribute to global supply chain resilience, global financial market stability, cooperation in the frontier technology sectors and science.Our three countries together have the best-in-class expertise in science and technology. And we are the ones who are implementing liberal democracies. Naturally, progress in science and technologies will bring benefits—tangible benefits—to our people, not just in terms of security, but also in terms of economy and science and technology.But what is most important here is not about our own interests only. When we put our forces together, I believe that we can make a contribution to the advancement of freedom and peace in the world. And that's exactly where our interests are aligned. I—look, back in May of 2022, I met with the families of the Japanese abductees during my visit, heard their stories, and empathized with them and got a sense of the pain they're feeling. It's real.We know there are many families out there who still wait and worry and wonder. And we're not going to forget about them or their loved ones.And there's clear language on this on our joint statement. The bottom line is this: That we share a common position; we're committed to working together to see the return of all prisoners of war and those who've been abducted and detained.And by the way, one of the things we get asked many times—and it wasn't directly asked, but implied—is what makes us think any of this is positive. Success brings success. When other nations see cooperation in the region, they make judgments about: Would they be better off if they made commitments? Will they move?Think about—as students of history, all of you; and you are—think about how many times successes have generated other successes when you don't anticipate it.And so I just think this is a—we're not going to forget, we're not giving up, and we're going to continue to make the case for the freedom of all of those detainees. With regard to your question for me, first of all, I have strong feelings about strengthening bilateral relationships between the R.O.K. and Japan. I share that with The two countries, in dealing with international challenges, should cooperate. We're both important neighboring countries. And so friendship with President Yoon and a relationship of trust, based on this, both countries as partners should open up a new era. And that is my thinking.This year, President Yoon came to Japan, and I visited the R.O.K. At international fora, we have repeated meetings—we have had repeated meetings. And between our two countries, including the economy and security, we've had forward-looking and concrete approaches which were started. It's already in motion, dynamically.Economic security dialogue was started—or it has been decided on. In the area of export control, there have been progress. And also Financial Ministers and Defense Ministers have had meetings.And so we've had this very positive, forward-looking developments. And these are seen not only in the public sector. Also in the private sector, we see a slate of developments, human exchanges, and exchanges between business circles. We're seeing very active developments in all of these areas, and that is a reality.Going forward, we hope to accumulate these approaches with—along with President Yoon to strengthen our bilateral relations even further. By generating results, we hope that people will understand Japan's feelings towards our bilateral relations. And we'd like to continue such efforts.Thank you. Thank you very much.Then, let me see. From Kyodo—Tajiri-san, Kyodo News. \\n\\n### Question ###\\n\\nFrom the perspective of the people of Korea, what would be the benefit felt from the strengthening of ties?\\n\\n Label: \""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.375\n",
      "Accuracy for label Direct Reply: 0.853\n",
      "Accuracy for label Indirect: 0.072\n",
      "Accuracy for label Direct Non-Reply: 0.562\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Direct Reply       0.38      0.85      0.53       218\n",
      "        Indirect       0.54      0.07      0.13       389\n",
      "Direct Non-Reply       0.30      0.56      0.39        73\n",
      "\n",
      "        accuracy                           0.38       680\n",
      "       macro avg       0.41      0.50      0.35       680\n",
      "    weighted avg       0.46      0.38      0.28       680\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[186  12  20]\n",
      " [284  28  77]\n",
      " [ 20  12  41]]\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = [label.replace(\"none\", \"Indirect\") for label in y_pred]\n",
    "y_pred1\n",
    "evaluate(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5n1KZofVGTZFVLtQb7p3Q",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
