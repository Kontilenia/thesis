{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          pipeline,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          PreTrainedTokenizer)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import huggingface_hub\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix)\n",
    "from typing import List\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install -U bitsandbytes\n",
    "!pip install huggingface-hub\n",
    "!pip install datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/ec2-user/SageMaker\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JF6TQ3ctdtsJ"
   },
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for QEvasion Dataset.\n",
    "\n",
    "    Attributes:\n",
    "        texts (List[str]): A list of text samples to be tokenized.\n",
    "        tokenizer (PreTrainedTokenizer): A tokenizer from the Hugging Face Transformers library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 tokenizer: PreTrainedTokenizer) -> None:\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "\n",
    "def create_prompted_text(dataset: pd.DataFrame,\n",
    "                         label_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates prompted text for classification from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing interview questions and \n",
    "        answers.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of formatted prompt texts for each interview response.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    global class_names\n",
    "    class_names = list(dataset[label_name].unique())\n",
    "    class_names_text = ', '.join(class_names)\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview. \"\n",
    "            f\"Classify the response to the selected question \"\n",
    "            f\"into one of the following categories: {class_names_text}\"\n",
    "            # f\". \\n Respond with only the category name.\\n\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel: {row[label_name]}\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_qevasion_dataset(tokenizer: PreTrainedTokenizer,\n",
    "                          label_name: str = \"clarity_label\") -> tuple:\n",
    "    \"\"\"\n",
    "    Loads the QEvasion dataset, splits it into training and validation sets,\n",
    "    and creates prompted texts for both sets.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The tokenizer to be used for text encoding.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get train set data\n",
    "    df = pd.read_csv('preprocessed_data/train_set.csv')[['question',\n",
    "                                                         'interview_question',\n",
    "                                                         'interview_answer',\n",
    "                                                         label_name]]\n",
    "\n",
    "    # Split train set to train and validation data\n",
    "    np.random.seed(2024)\n",
    "    msk = np.random.rand(len(df)) < 0.9\n",
    "    train = df[msk]\n",
    "    validation = df[~msk]\n",
    "\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_texts = create_prompted_text(train, label_name)\n",
    "    validation_texts = create_prompted_text(validation,\n",
    "                                            label_name)\n",
    "\n",
    "    # print(\"Example of train test:\" + train_texts[1])\n",
    "    # print(\"Example of validation test:\" + validation_texts[1])\n",
    "\n",
    "    train_texts = train_texts # [:20]\n",
    "    validation_texts = validation_texts # [:2]\n",
    "    return (CustomTextDataset(train_texts, tokenizer),\n",
    "            CustomTextDataset(validation_texts, tokenizer))\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model for which to count trainable parameters.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"\"\"trainable params: {trainable_params} || all params: {all_param}\n",
    "        || trainable%: {100 * trainable_params / all_param}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def finetuning(model_name: str,\n",
    "               output_model_dir: str,\n",
    "               label_taxonomy: str,\n",
    "               lr: float,\n",
    "               epochs: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Fine-tunes a pre-trained language model with LoRA.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model.\n",
    "        output_model_dir (str): Directory to save the fine-tuned model.\n",
    "        label_taxonomy (str): The label taxonomy for the dataset.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The fine-tuned model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True,)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # !TODO: Check if this helps\n",
    "    # tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            # Cast the small parameters to fp32 for stability\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "    # Reduce number of stored activation\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,  # Attention heads\n",
    "        lora_alpha=32,  # Alpha scaling\n",
    "        # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Load data\n",
    "    train_data, validation_data = load_qevasion_dataset(tokenizer,\n",
    "                                                        label_taxonomy)\n",
    "\n",
    "    print(f\"\"\"Found {len(train_data)} instances for training and\n",
    "    {len(validation_data) } instances for validation.\"\"\")\n",
    "\n",
    "    grad_accum_steps = 8\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training...\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=validation_data,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=grad_accum_steps,\n",
    "            eval_accumulation_steps=1,\n",
    "            warmup_steps=100,\n",
    "            max_steps=int((len(train_data)*epochs)/grad_accum_steps),\n",
    "            learning_rate=lr,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            # eval_steps * int((len(train_data)*epochs)/grad_accum_steps)\n",
    "            # if eval_steps < 1\n",
    "            eval_steps=0.33 / epochs,\n",
    "            eval_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            report_to=\"wandb\",\n",
    "            # save_steps= 2,\n",
    "            # num_train_epochs=epochs,\n",
    "            # output_dir=f'outputs_{out_dir}'\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                      mlm=False)\n",
    "    )\n",
    "\n",
    "    # Silence the warnings\n",
    "    model.config.use_cache = False\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(output_model_dir)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def create_test_prompted_text(dataset: pd.DataFrame,\n",
    "                              label_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates prompted text for classification from the test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing interview questions\n",
    "        and answers.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted prompt texts for each interview response.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel:\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def create_inference_prompted_text(dataset: pd.DataFrame,\n",
    "                                   label_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates prompted text for classification from the test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing interview questions\n",
    "        and answers.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted prompt texts for each interview response.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel:\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def predict(test: pd.DataFrame,\n",
    "            categories: list,\n",
    "            model: nn.Module,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            after_training: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Generates predictions for the test dataset using the provided model\n",
    "    and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        test (pd.DataFrame): The test dataset containing prompts.\n",
    "        categories (list): The list of possible categories for classification.\n",
    "        model (nn.Module): The trained model for making predictions.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted labels for the test dataset.\n",
    "    \"\"\"\n",
    "    batch_size = 8\n",
    "    category_set = set(category.lower() for category in categories)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        y_pred = []\n",
    "\n",
    "        if after_training:\n",
    "            pipe = pipeline(task=\"text-generation\",\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_new_tokens=5,\n",
    "                            temperature=0.1)\n",
    "        else:\n",
    "            pipe = pipeline(task=\"text-generation\",\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            temperature=0.1)\n",
    "\n",
    "        terminators = [\n",
    "            pipe.tokenizer.eos_token_id,\n",
    "            pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        for i in tqdm(range(0, len(test), batch_size)):\n",
    "            prompts = test.iloc[i:i + batch_size][\"text\"].tolist()\n",
    "            results = pipe(prompts,\n",
    "                           eos_token_id=terminators)\n",
    "\n",
    "            for result in results:\n",
    "                answer = result[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "                matched = False\n",
    "\n",
    "                for category in category_set:\n",
    "                    if category in answer.lower():\n",
    "                        print(f\"Right label: {answer.lower()}\")\n",
    "                        y_pred.append(category)\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "                if not matched:\n",
    "                    print(f\"Wrong label: {answer.lower()}\")\n",
    "                    y_pred.append(\"none\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluation_report(y_true: pd.Series,\n",
    "                      y_pred: pd.Series,\n",
    "                      labels: list,\n",
    "                      run=None) -> None:\n",
    "    \"\"\"\n",
    "    Generates and prints an evaluation report including accuracy and \n",
    "    classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): The true labels for the test dataset.\n",
    "        y_pred (np.ndarray): The predicted labels for the test dataset.\n",
    "        labels (list): The list of label names.\n",
    "        run: Optional; a wandb run object for logging metrics.\n",
    "    \"\"\"\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict = {}\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    if run:\n",
    "        wandb_log_dict[\"Accuracy\"] = accuracy\n",
    "\n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped))\n",
    "                         if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.2f}')\n",
    "        if run:\n",
    "            wandb_log_dict[f\"Accuracy for label {labels[label]}\"] = label_accuracy\n",
    "\n",
    "    unsplit_labels = [label.replace(\" \", \"_\") for label in labels]\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped,\n",
    "                                         y_pred=y_pred_mapped,\n",
    "                                         target_names=unsplit_labels,\n",
    "                                         labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    report_columns = [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    report_table = []\n",
    "    class_report = class_report.splitlines()\n",
    "    for line in class_report[2:(len(labels)+2)]:\n",
    "        report_table.append(line.split())\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict[\"Classification Report\"] = wandb.Table(\n",
    "            data=report_table,\n",
    "            columns=report_columns)\n",
    "\n",
    "    # For not predicted classes\n",
    "    mask = y_pred_mapped != -1\n",
    "    y_true_mapped2 = y_true_mapped[mask]\n",
    "    y_pred_mapped2 = y_pred_mapped[mask]\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped,\n",
    "                                   y_pred=y_pred_mapped,\n",
    "                                   labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict[\"Confusion Matix\"] = wandb.plot.confusion_matrix(\n",
    "            y_true=y_true_mapped2,\n",
    "            preds=y_pred_mapped2,\n",
    "            class_names=labels\n",
    "        )\n",
    "        run.log(wandb_log_dict)\n",
    "\n",
    "def create_labels_train_set(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create labels for the dataset\n",
    "\n",
    "    Arguments:\n",
    "        df: Dataframe\n",
    "\n",
    "    Returns:\n",
    "        df: Dataframe with labels\n",
    "    \"\"\"\n",
    "\n",
    "    clarity_mapping ={\n",
    "    'explicit': 'direct reply',\n",
    "    'implicit': 'indirect',\n",
    "    'dodging': \"indirect\",\n",
    "    'deflection': \"indirect\",\n",
    "    'partial/half-answer': \"indirect\",\n",
    "    'general': \"indirect\",\n",
    "    'contradictory': \"indirect\",\n",
    "    'declining_to_answer': \"direct non-reply\",\n",
    "    'claims_ignorance': \"direct non-reply\",\n",
    "    'clarification': \"direct non-reply\",\n",
    "    'diffusion': \"indirect\",\n",
    "    }\n",
    "    \n",
    "    df[\"clarity_label\"] = df[\"label\"].map(clarity_mapping)\n",
    "    df.rename(columns={\"label\": \"evasion_label\"}, inplace=True)\n",
    "    return df\n",
    "    \n",
    "# !TODO: Make the logic of the function better\n",
    "def evaluate(base_model_name: str,\n",
    "             fine_tuned_model_path: str,\n",
    "             train_label_name: str,\n",
    "             test_label_name: str,\n",
    "             test_set_path: str = 'preprocessed_data/test_set.csv',\n",
    "             model: nn.Module = None,\n",
    "             tokenizer: PreTrainedTokenizer = None,\n",
    "             run=None) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the fine-tuned model on the test dataset and\n",
    "    generates an evaluation report.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): The name of the base model to load.\n",
    "        fine_tuned_model_path (str): The path to the fine-tuned model.\n",
    "        train_label_name (str): The name of the label column for classification\n",
    "        in train set.\n",
    "        test_label_name (str): The name of the label column for classification\n",
    "        in test set.\n",
    "        model (nn.Module): Optional; a pre-trained model to use for evaluation.\n",
    "        tokenizer (PreTrainedTokenizer): Optional; a tokenizer to use for \n",
    "        evaluation.\n",
    "        run: Optional; a wandb run object for logging metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    if not model:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            fine_tuned_model_path,\n",
    "            return_dict=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "            # torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            offload_folder=\"offload/\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "    if not tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "                                                  cache_dir=cache_dir)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # Get test set data\n",
    "    test_df = pd.read_csv(test_set_path)[[\n",
    "        'question',\n",
    "        'interview_question',\n",
    "        'interview_answer',\n",
    "        test_label_name,\n",
    "        train_label_name\n",
    "    ]]\n",
    "\n",
    "    # creating bool series False for NaN values\n",
    "    test_df = test_df[test_df[\"evasion_label\"].notnull()]\n",
    "\n",
    "    test_texts = create_test_prompted_text(test_df, train_label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    # NEW\n",
    "    labels = [label.lower() for label in list(test_df[train_label_name].unique())]\n",
    "    test_labels = [label.lower() for label in list(test_df[test_label_name].unique())]\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer, True)\n",
    "    y_pred = pd.DataFrame({\"label\": y_pred})\n",
    "    y_pred_evasion_based = create_labels_train_set(y_pred)[test_label_name]\n",
    "    print(\"----\")\n",
    "    print(y_pred_evasion_based)\n",
    "    print(\"----\")\n",
    "    y_true = test_df[test_label_name].str.lower()\n",
    "    print(\"----\")\n",
    "    print(y_true)\n",
    "    print(\"----\")\n",
    "    print(test_labels)\n",
    "    evaluation_report(y_true, y_pred_evasion_based, test_labels, run)\n",
    "\n",
    "    # # Get train set data\n",
    "    # train_df = pd.read_csv('preprocessed_data/train_set.csv')[[\n",
    "    #     'question',\n",
    "    #     'interview_question',\n",
    "    #     'interview_answer',\n",
    "    #     train_label_name\n",
    "    # ]] # [:20]\n",
    "\n",
    "    # np.random.seed(2024)\n",
    "    # msk = np.random.rand(len(train_df)) < 0.9\n",
    "    # validation_df = train_df[~msk]\n",
    "\n",
    "    # train_texts = create_test_prompted_text(validation_df, train_label_name)\n",
    "    # dataset = pd.DataFrame(train_texts, columns=['text'])\n",
    "\n",
    "    # labels = [label.lower() for label in list(validation_df[train_label_name].unique())]\n",
    "\n",
    "    # y_pred = predict(dataset, labels, model, tokenizer, True)\n",
    "    # y_pred = pd.Series(y_pred, name=train_label_name)\n",
    "    # y_true = validation_df[train_label_name].str.lower()\n",
    "    # evaluation_report(y_true, y_pred, labels, run)\n",
    "\n",
    "\n",
    "def inference(base_model_name: str,\n",
    "              fine_tuned_model_path: str,\n",
    "              label_name: str,\n",
    "              model: nn.Module = None,\n",
    "              tokenizer: PreTrainedTokenizer = None,\n",
    "              ) -> None:\n",
    "    \"\"\"\n",
    "    Inference a model on the test dataset and\n",
    "    generates an evaluation report.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): The name of the base model to load.\n",
    "        fine_tuned_model_path (str): The path to the fine-tuned model.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        model (nn.Module): Optional; a pre-trained model to use for inference.\n",
    "        tokenizer (PreTrainedTokenizer): Optional; a tokenizer to use for inference.\n",
    "    \"\"\"\n",
    "\n",
    "    if not model:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            return_dict=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "            # torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            offload_folder=\"offload/\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "    if not tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "                                                  cache_dir=cache_dir)\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # Get test set data\n",
    "    test_df = pd.read_csv('preprocessed_data/test_set.csv')[[\n",
    "        'question',\n",
    "        'interview_question',\n",
    "        'interview_answer',\n",
    "        label_name\n",
    "    ]]  # [:20]\n",
    "\n",
    "    test_texts = create_inference_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = [label.lower() for label in list(test_df[label_name].unique())]\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_pred = pd.Series(y_pred, name=label_name)\n",
    "    y_true = test_df[label_name].str.lower()\n",
    "    evaluation_report(y_true, y_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fine-tuning and evaluation                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'lora_llm'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "\n",
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    huggingface_hub.login(secrets.get('HF_KEY'))\n",
    "    wandb.login(key=secrets.get('WANDB_KEY'))\n",
    "\n",
    "lr = 2e-4\n",
    "epochs = 7\n",
    "class_names = []\n",
    "base_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "label_name = \"evasion_label\"\n",
    "fine_tuned_model_path = f\"./llama3.1_{epochs}ep\"\n",
    "\n",
    "# Wandb configuration\n",
    "run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "                 project='political-speech-clarity',\n",
    "                 job_type=\"training\",\n",
    "                 name=str(epochs)+\" epoch Llama3.1 8B 1/3 validation\",\n",
    "                 # Track hyperparameters and run metadata\n",
    "                 config={\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"architecture\": base_model_name,\n",
    "                    \"dataset\": \"qevasion_dataset_preproccessed\",\n",
    "                    \"epochs\": epochs,\n",
    "                 })\n",
    "\n",
    "# run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "#                  project=\"political-speech-clarity\",\n",
    "#                  id=\"dnby3t8o\",\n",
    "#                  resume=\"must\")\n",
    "\n",
    "# model, tokenizer = finetuning(base_model_name,\n",
    "#                               fine_tuned_model_path,\n",
    "#                               label_name,\n",
    "#                               lr,\n",
    "#                               epochs)\n",
    "\n",
    "evaluate(base_model_name,\n",
    "         fine_tuned_model_path,\n",
    "         \"evasion_label\",\n",
    "         \"clarity_label\",\n",
    "         # model,\n",
    "         # tokenizer,\n",
    "         run=run)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt tuning and model evaluation                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    huggingface_hub.login(secrets.get('HF_KEY'))\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    cache_dir=cache_dir,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inference(base_model_name,\n",
    "          \"\",\n",
    "          \"clarity_label\",\n",
    "          model,\n",
    "          tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5n1KZofVGTZFVLtQb7p3Q",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
