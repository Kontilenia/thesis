{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          pipeline,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          PreTrainedTokenizer)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import huggingface_hub\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix)\n",
    "from typing import List\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/ec2-user/SageMaker\"\n",
    "os.environ['HF_HOME'] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install -U bitsandbytes\n",
    "!pip install huggingface-hub\n",
    "!pip install datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JF6TQ3ctdtsJ"
   },
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for QEvasion Dataset.\n",
    "\n",
    "    Attributes:\n",
    "        texts (List[str]): A list of text samples to be tokenized.\n",
    "        tokenizer (PreTrainedTokenizer): A tokenizer from the Hugging Face Transformers library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 tokenizer: PreTrainedTokenizer) -> None:\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "\n",
    "def create_prompted_text(dataset: pd.DataFrame,\n",
    "                         label_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates prompted text for classification from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing interview questions and \n",
    "        answers.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of formatted prompt texts for each interview response.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    global class_names\n",
    "    class_names = list(dataset[label_name].unique())\n",
    "    class_names_text = ', '.join(class_names)\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview. \"\n",
    "            f\"Classify the response to the selected question \"\n",
    "            f\"into one of the following categories: {class_names_text}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel: {row[label_name]}\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_qevasion_dataset(tokenizer: PreTrainedTokenizer,\n",
    "                          label_name: str = \"clarity_label\") -> tuple:\n",
    "    \"\"\"\n",
    "    Loads the QEvasion dataset, splits it into training and validation sets,\n",
    "    and creates prompted texts for both sets.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The tokenizer to be used for text encoding.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get train set data\n",
    "    df = pd.read_csv('preprocessed_data/train_set.csv')[['question',\n",
    "                                                         'interview_question',\n",
    "                                                         'interview_answer',\n",
    "                                                         label_name]]\n",
    "\n",
    "    # Split train set to train and validation data\n",
    "    np.random.seed(2024)\n",
    "    msk = np.random.rand(len(df)) < 0.9\n",
    "    train = df[msk]\n",
    "    validation = df[~msk]\n",
    "\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_texts = create_prompted_text(train, label_name)\n",
    "    validation_texts = create_prompted_text(validation,\n",
    "                                            label_name)\n",
    "\n",
    "    # print(\"Example of train test:\" + train_texts[1])\n",
    "    # print(\"Example of validation test:\" + validation_texts[1])\n",
    "\n",
    "    train_texts = train_texts\n",
    "    validation_texts = validation_texts\n",
    "    return (CustomTextDataset(train_texts, tokenizer),\n",
    "            CustomTextDataset(validation_texts, tokenizer))\n",
    "\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     references = pred.label_ids\n",
    "#     generated_ids = pred.predictions.argmax(axis=-1)  # Get the predicted token IDs\n",
    "#     generated_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]  # Decode to text\n",
    "#     print(\"----\")\n",
    "#     print(\"References:\", references)\n",
    "#     print(\"Generated Texts:\", generated_texts)\n",
    "#     print(\"----\")\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model for which to count trainable parameters.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"\"\"trainable params: {trainable_params} || all params: {all_param}\n",
    "        || trainable%: {100 * trainable_params / all_param}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def finetuning(model_name: str,\n",
    "               output_model_dir: str,\n",
    "               label_taxonomy: str,\n",
    "               lr: float,\n",
    "               epochs: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Fine-tunes a pre-trained language model with LoRA.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model.\n",
    "        output_model_dir (str): Directory to save the fine-tuned model.\n",
    "        label_taxonomy (str): The label taxonomy for the dataset.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The fine-tuned model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True,)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # !TODO: Check if this helps\n",
    "    # tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            # Cast the small parameters to fp32 for stability\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "    # Reduce number of stored activation\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,  # Attention heads\n",
    "        lora_alpha=32,  # Alpha scaling\n",
    "        # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Load data\n",
    "    train_data, validation_data = load_qevasion_dataset(tokenizer,\n",
    "                                                        label_taxonomy)\n",
    "\n",
    "    print(f\"\"\"Found {len(train_data)} instances for training and\n",
    "    {len(validation_data) } instances for validation.\"\"\")\n",
    "\n",
    "    grad_accum_steps = 8\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training...\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=validation_data,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=grad_accum_steps,\n",
    "            eval_accumulation_steps=1,\n",
    "            warmup_steps=100,\n",
    "            max_steps=int((len(train_data)*epochs)/grad_accum_steps),\n",
    "            learning_rate=lr,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            # eval_steps * int((len(train_data)*epochs)/grad_accum_steps)\n",
    "            # if eval_steps < 1\n",
    "            eval_steps=0.33 / epochs,\n",
    "            eval_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            report_to=\"wandb\",\n",
    "            # save_steps= 2,\n",
    "            # num_train_epochs=epochs,\n",
    "            # output_dir=f'outputs_{out_dir}'\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                      mlm=False)\n",
    "    )\n",
    "\n",
    "    # Silence the warnings\n",
    "    model.config.use_cache = False\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(output_model_dir)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def create_test_prompted_text(dataset: pd.DataFrame,\n",
    "                              label_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates prompted text for classification from the test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing interview questions\n",
    "        and answers.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted prompt texts for each interview response.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel:\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def create_inference_prompted_text(dataset: pd.DataFrame,\n",
    "                                   label_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates prompted text for classification from the test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing interview questions\n",
    "        and answers.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted prompt texts for each interview response.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel:\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def predict(test: pd.DataFrame,\n",
    "            categories: list,\n",
    "            model: nn.Module,\n",
    "            tokenizer: PreTrainedTokenizer) -> list:\n",
    "    \"\"\"\n",
    "    Generates predictions for the test dataset using the provided model\n",
    "    and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        test (pd.DataFrame): The test dataset containing prompts.\n",
    "        categories (list): The list of possible categories for classification.\n",
    "        model (nn.Module): The trained model for making predictions.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted labels for the test dataset.\n",
    "    \"\"\"\n",
    "    batch_size = 8\n",
    "    category_set = set(category.lower() for category in categories)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        y_pred = []\n",
    "        pipe = pipeline(task=\"text-generation\",\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer)\n",
    "\n",
    "        for i in tqdm(range(0, len(test), batch_size)):\n",
    "            prompts = test.iloc[i:i + batch_size][\"text\"].tolist()\n",
    "            results = pipe(prompts)\n",
    "\n",
    "            for result in results:\n",
    "                answer = result[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "                matched = False\n",
    "\n",
    "                for category in category_set:\n",
    "                    if category in answer.lower():\n",
    "                        print(f\"Right label: {answer.lower()}\")\n",
    "                        y_pred.append(category)\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "                if not matched:\n",
    "                    print(f\"Wrong label: {answer.lower()}\")\n",
    "                    y_pred.append(\"none\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluation_report(y_true: pd.Series,\n",
    "                      y_pred: pd.Series,\n",
    "                      labels: list,\n",
    "                      run=None) -> None:\n",
    "    \"\"\"\n",
    "    Generates and prints an evaluation report including accuracy and \n",
    "    classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): The true labels for the test dataset.\n",
    "        y_pred (np.ndarray): The predicted labels for the test dataset.\n",
    "        labels (list): The list of label names.\n",
    "        run: Optional; a wandb run object for logging metrics.\n",
    "    \"\"\"\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict = {}\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    if run:\n",
    "        wandb_log_dict[\"Accuracy\"] = accuracy\n",
    "\n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped))\n",
    "                         if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.2f}')\n",
    "        if run:\n",
    "            wandb_log_dict[f\"Accuracy for label {labels[label]}\"] = label_accuracy\n",
    "\n",
    "    unsplit_labels = [label.replace(\" \", \"_\") for label in labels]\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped,\n",
    "                                         y_pred=y_pred_mapped,\n",
    "                                         target_names=unsplit_labels,\n",
    "                                         labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    report_columns = [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    report_table = []\n",
    "    class_report = class_report.splitlines()\n",
    "    for line in class_report[2:(len(labels)+2)]:\n",
    "        report_table.append(line.split())\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict[\"Classification Report\"] = wandb.Table(\n",
    "            data=report_table,\n",
    "            columns=report_columns)\n",
    "\n",
    "    # For not predicted classes\n",
    "    mask = y_pred_mapped != -1\n",
    "    y_true_mapped2 = y_true_mapped[mask]\n",
    "    y_pred_mapped2 = y_pred_mapped[mask]\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped,\n",
    "                                   y_pred=y_pred_mapped,\n",
    "                                   labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict[\"Confusion Matix\"] = wandb.plot.confusion_matrix(\n",
    "            y_true=y_true_mapped2,\n",
    "            preds=y_pred_mapped2,\n",
    "            class_names=labels\n",
    "        )\n",
    "        run.log(wandb_log_dict)\n",
    "\n",
    "\n",
    "# !TODO: Load model from file\n",
    "def evaluate(base_model_name: str,\n",
    "             fine_tuned_model_path: str,\n",
    "             label_name: str,\n",
    "             model: nn.Module = None,\n",
    "             tokenizer: PreTrainedTokenizer = None,\n",
    "             run=None) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the fine-tuned model on the test dataset and\n",
    "    generates an evaluation report.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): The name of the base model to load.\n",
    "        fine_tuned_model_path (str): The path to the fine-tuned model.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        model (nn.Module): Optional; a pre-trained model to use for evaluation.\n",
    "        tokenizer (PreTrainedTokenizer): Optional; a tokenizer to use for evaluation.\n",
    "        run: Optional; a wandb run object for logging metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    if not model:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            return_dict=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "            # torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            offload_folder=\"offload/\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "    if not tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "                                                  cache_dir=cache_dir)\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # Get test set data\n",
    "    test_df = pd.read_csv('preprocessed_data/test_set.csv')[[\n",
    "        'question',\n",
    "        'interview_question',\n",
    "        'interview_answer',\n",
    "        label_name\n",
    "    ]]  # [:20]\n",
    "\n",
    "    test_texts = create_test_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = [label.lower() for label in list(test_df[label_name].unique())]\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_pred = pd.Series(y_pred, name=label_name)\n",
    "    y_true = test_df[label_name].str.lower()\n",
    "    evaluation_report(y_true, y_pred, labels, run)\n",
    "\n",
    "    test_texts = create_test_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = [label.lower() for label in list(test_df[label_name].unique())]\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_pred = pd.Series(y_pred, name=label_name)\n",
    "    y_true = test_df[label_name].str.lower()\n",
    "    evaluation_report(y_true, y_pred, labels, run)\n",
    "\n",
    "\n",
    "# !TODO: Load model from file\n",
    "def inference(base_model_name: str,\n",
    "              fine_tuned_model_path: str,\n",
    "              label_name: str,\n",
    "              model: nn.Module = None,\n",
    "              tokenizer: PreTrainedTokenizer = None,\n",
    "              ) -> None:\n",
    "    \"\"\"\n",
    "    Inference a model on the test dataset and\n",
    "    generates an evaluation report.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): The name of the base model to load.\n",
    "        fine_tuned_model_path (str): The path to the fine-tuned model.\n",
    "        label_name (str): The name of the label column for classification.\n",
    "        model (nn.Module): Optional; a pre-trained model to use for inference.\n",
    "        tokenizer (PreTrainedTokenizer): Optional; a tokenizer to use for inference.\n",
    "    \"\"\"\n",
    "\n",
    "    if not model:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            return_dict=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "            # torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            offload_folder=\"offload/\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "    if not tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "                                                  cache_dir=cache_dir)\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # Get test set data\n",
    "    test_df = pd.read_csv('preprocessed_data/test_set.csv')[[\n",
    "        'question',\n",
    "        'interview_question',\n",
    "        'interview_answer',\n",
    "        label_name\n",
    "    ]]  # [:20]\n",
    "\n",
    "    test_texts = create_inference_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = [label.lower() for label in list(test_df[label_name].unique())]\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_pred = pd.Series(y_pred, name=label_name)\n",
    "    y_true = test_df[label_name].str.lower()\n",
    "    evaluation_report(y_true, y_pred, labels)\n",
    "\n",
    "    test_texts = create_test_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = [label.lower() for label in list(test_df[label_name].unique())]\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_pred = pd.Series(y_pred, name=label_name)\n",
    "    y_true = test_df[label_name].str.lower()\n",
    "    evaluation_report(y_true, y_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fine-tuning and evaluation                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find lora_llm.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkontilenia\u001b[0m (\u001b[33mkontilenia-national-technical-university-of-athens\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/thesis/wandb/run-20250611_164914-j3jd3s65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity/runs/j3jd3s65' target=\"_blank\">8 epoch Llama3.1 8b</a></strong> to <a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity' target=\"_blank\">https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity/runs/j3jd3s65' target=\"_blank\">https://wandb.ai/kontilenia-national-technical-university-of-athens/political-speech-clarity/runs/j3jd3s65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732dc281f47343efa788d4c0ca98c942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6815744 || all params: 4547416064\n",
      "        || trainable%: 0.14988168894325302\n",
      "Found 3109 instances for training and\n",
      "    339 instances for validation.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='646' max='3109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 646/3109 1:47:41 < 6:51:53, 0.10 it/s, Epoch 1.66/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.804900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2.039700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>1.869200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>1.774700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/339 02:03 < 00:13, 2.45 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50df60114a1b4cbd9e47abeeb8fd3c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'lora_llm'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "\n",
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    huggingface_hub.login(secrets.get('HF_KEY'))\n",
    "    wandb.login(key=secrets.get('WANDB_KEY'))\n",
    "\n",
    "base_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "fine_tuned_model_path = \"./llama3.1\"\n",
    "label_name = \"evasion_label\"\n",
    "lr = 2e-4\n",
    "epochs = 8\n",
    "class_names = []\n",
    "\n",
    "# Wandb configuration\n",
    "run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "                 project='political-speech-clarity',\n",
    "                 job_type=\"training\",\n",
    "                 name=str(epochs)+\" epoch Llama3.1 8b\",\n",
    "                 # Track hyperparameters and run metadata\n",
    "                 config={\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"architecture\": base_model_name,\n",
    "                    \"dataset\": \"qevasion_dataset_preproccessed\",\n",
    "                    \"epochs\": epochs,\n",
    "                 })\n",
    "\n",
    "model, tokenizer = finetuning(base_model_name,\n",
    "                              fine_tuned_model_path,\n",
    "                              label_name,\n",
    "                              lr,\n",
    "                              epochs)\n",
    "\n",
    "# run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "#                  project=\"political-speech-clarity\",\n",
    "#                  id=\"8c0qfc9s\",\n",
    "#                  resume=\"must\")\n",
    "\n",
    "evaluate(base_model_name,\n",
    "         fine_tuned_model_path,\n",
    "         \"clarity_label\",\n",
    "         model,\n",
    "         tokenizer,\n",
    "         run)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt tuning and model evaluation                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  3%|▎         | 1/39 [01:50<1:09:40, 110.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right label: indirect \n",
      "reason: the response did not directly answer the question about the redline, but instead diverted the topic to a broader discussion about the international response to the north korean regime's actions. \n",
      "\n",
      "### explanation of labels: \n",
      "- indirect: the response does not directly answer the question, often diverting the topic or providing a vague statement. \n",
      "- direct reply: the response clearly and concisely answers the question, providing the requested information. \n",
      "- direct non-reply: the response explicitly states that the question will not be answered or provides a clear refusal to comment.\n",
      "Right label: direct reply \n",
      "\n",
      "explanation: the response provides a clear and direct answer to the question, stating the conditions under which they would invite others to the white house to negotiate on the jobs bill.\n",
      "Right label: direct non-reply\n",
      "\n",
      "reason: the response did not directly address the question about the necessity of japan dropping the threat of sanctions from its proposed security council resolution. instead, it provided a broader context about the purpose of the u.n. security council resolution and the diplomatic efforts surrounding north korea's actions.\n",
      "Right label: indirect reply \n",
      "\n",
      "rationale: the respondent did not answer the question directly and instead provided a general overview of the ongoing diplomatic efforts and the complexity of the situation. the respondent also deflected the question to another person (condi), and did not provide a specific timeline for when the resolution will be seen. the respondent's answer was indirect and did not provide a clear answer to the question.\n",
      "Right label: indirect \n",
      "\n",
      "reason: the president did not provide a direct answer to the question. he stated that he doesn't consider the report credible and then went on to talk about the violence in iraq and the need for the iraqi government to improve security. he didn't provide an updated figure or directly address the discrepancy between his previous statement and the report.\n",
      "Right label: direct non-reply \n",
      "\n",
      "reason: the speaker does not answer the question about whether the republican leader owes an apology. instead, they discuss the difference in attitude between the parties, using the patriot act as an example, and emphasize that they do not question the patriotism of those who disagree with them. they also express confidence that the leader meant nothing personal and shares their concern about passing good legislation. the speaker avoids a direct answer to the question about an apology.\n",
      "Right label: direct non-reply.\n",
      "Right label: indirect \n",
      "\n",
      "rationale: the response does not directly address the question of whether the demand for israeli troops to pull out immediately is negotiable. instead, it provides a broader context and discusses the importance of addressing the root causes of the problem, supporting democracies, and preventing a vacuum that could be exploited by hizballah and its sponsors. the response also touches on the idea that the lebanese government should move into the south and that there should be an international force to provide help, but it does not explicitly state whether the demand for an immediate pullout is negotiable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/39 [03:11<57:40, 93.52s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right label: direct non-reply\n",
      "reason: the question asks what the speaker is offering arab nations in order to encourage them to participate. however, the speaker's response does not address the issue of what is being offered. instead, the speaker talks about the importance of arab buy-in for a state and the need for the arab nations to be part of the process. the speaker does not provide a direct answer to the question.\n",
      "Right label: direct non-reply \n",
      "\n",
      "explanation: the response does not address the question directly. the president avoids providing a straightforward answer to whether tehran's influence is growing despite us efforts, instead focusing on the broader threat of terrorism, the importance of international cooperation, and the need to advance liberty and defeat an ideology that doesn't believe in freedom. the response includes some relevant information, but it does not provide a clear answer to the question.\n",
      "Right label: indirect \n",
      "\n",
      "reason: the response did not answer the question directly and instead discussed the president's relationship with putin and areas of agreement and disagreement.\n",
      "Right label: indirect \n",
      "\n",
      "reason: the response does not directly answer the question, instead the speaker explains that the topic will be discussed at a meeting and that they will have a better answer after that meeting.\n",
      "Right label: direct non-reply\n",
      "\n",
      "the response does not address the question of whether the president sees any contradictory evidence in the culture, instead continuing to elaborate on his thoughts about the third awakening and the changing culture.\n",
      "Right label: direct reply\n",
      "\n",
      "the response directly addresses the question, providing a detailed account of the achievements and progress made during their 8-year cooperation, including the improvement in u.s.-russia relations, cooperation in counterterrorism, fighting proliferation, and economic cooperation. the response also explicitly states what they will leave behind for their successors, indicating a direct reply to the question.\n",
      "Right label: direct non-reply.\n",
      "Right label: **direct reply**. the response directly answers the question by stating the messages for the governments of iran and syria.\n"
     ]
    }
   ],
   "source": [
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    huggingface_hub.login(secrets.get('HF_KEY'))\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    cache_dir=cache_dir,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inference(base_model_name,\n",
    "          \"\",\n",
    "          \"clarity_label\",\n",
    "          model,\n",
    "          tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        374G     0  374G   0% /dev\n",
      "tmpfs           374G   40K  374G   1% /dev/shm\n",
      "tmpfs           374G  1.5M  374G   1% /run\n",
      "tmpfs           374G     0  374G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  135G   82G   54G  61% /\n",
      "tmpfs            75G     0   75G   0% /run/user/0\n",
      "/dev/nvme3n1    2.0T  142G  1.8T   8% /home/ec2-user/SageMaker\n",
      "tmpfs            75G     0   75G   0% /run/user/1000\n",
      "tmpfs            75G     0   75G   0% /run/user/1002\n",
      "tmpfs            75G     0   75G   0% /run/user/1001\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5n1KZofVGTZFVLtQb7p3Q",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
