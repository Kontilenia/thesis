{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          pipeline,\n",
    "                          DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import huggingface_hub\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "# import wandb\n",
    "# import argparse\n",
    "# from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install -U bitsandbytes\n",
    "!pip install huggingface-hub\n",
    "!pip install datasets\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JF6TQ3ctdtsJ"
   },
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "\n",
    "def create_prompted_text(\n",
    "    dataset,\n",
    "    label_name,\n",
    "):\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview. \"\n",
    "            f\"Classify the response to the selected question \"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel: {row[label_name]}\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_qevasion_dataset(\n",
    "    tokenizer,\n",
    "    label_name=\"clarity_label\"\n",
    "):\n",
    "    # Get train set data\n",
    "    df = pd.read_csv('preprocessed_data/train_set.csv')[['question',\n",
    "                                                         'interview_question',\n",
    "                                                         'interview_answer',\n",
    "                                                         label_name]]\n",
    "\n",
    "    # Split train set to train and validation data\n",
    "    np.random.seed(2024)\n",
    "    msk = np.random.rand(len(df)) < 0.9\n",
    "    train = df[msk]\n",
    "    validation = df[~msk]\n",
    "\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_texts = create_prompted_text(train, label_name)\n",
    "    validation_texts = create_prompted_text(validation,\n",
    "                                            label_name)\n",
    "\n",
    "    print(\"Example of train test:\" + train_texts[1])\n",
    "    print(\"Example of validation test:\" + validation_texts[1])\n",
    "\n",
    "    train_texts = train_texts[:8]\n",
    "    validation_texts = validation_texts[:1]\n",
    "    return (CustomTextDataset(train_texts, tokenizer),\n",
    "            CustomTextDataset(validation_texts, tokenizer))\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"\"\"trainable params: {trainable_params} || all params: {all_param}\n",
    "        || trainable%: {100 * trainable_params / all_param}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def finetuning(model_name,\n",
    "               output_model_dir,\n",
    "               label_taxonomy,\n",
    "               lr,\n",
    "               epochs):\n",
    "\n",
    "    cache_dir = \"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True,)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "    # Reduce number of stored activation\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,  # Attention heads\n",
    "        lora_alpha=32,  # Alpha scaling\n",
    "        # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"  # set this for CLM or Seq2Seq\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # load data\n",
    "    train_data, validation_data = load_qevasion_dataset(tokenizer,\n",
    "                                                        label_taxonomy)\n",
    "\n",
    "    print(f\"\"\"Found {len(train_data)} instances for training and\n",
    "    {len(validation_data) } instances for validation.\"\"\")\n",
    "\n",
    "    grad_accum_steps = 8\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training...\")\n",
    "    # out_dir = output_model_dir.split(\"/\")[-1]\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=validation_data,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=grad_accum_steps,\n",
    "            eval_accumulation_steps=1,\n",
    "            warmup_steps=100,\n",
    "            max_steps=int((len(train_data)*epochs)/grad_accum_steps),\n",
    "            learning_rate=lr,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            # eval_steps * int((len(train_data)*epochs)/grad_accum_steps)\n",
    "            # if eval_steps < 1\n",
    "            eval_steps=0.33/epochs,\n",
    "            eval_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            # report_to=\"wandb\",\n",
    "            # save_steps= 2,\n",
    "            # num_train_epochs=epochs,\n",
    "            # output_dir=f'outputs_{out_dir}' #,\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                      mlm=False)\n",
    "    )\n",
    "\n",
    "    # silence the warnings. Re-enable for inference!\n",
    "    model.config.use_cache = False\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(output_model_dir)\n",
    "\n",
    "    # Optionally, save the tokenizer as well\n",
    "    # tokenizer.save_pretrained(output_model_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def create_test_prompted_text(\n",
    "    dataset,\n",
    "    label_name,\n",
    "):\n",
    "    texts = []\n",
    "    classes_names = ', '.join(list(dataset[label_name].unique()))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        texts.append(\n",
    "            f\"You will be given a part of an interview.\"\n",
    "            f\"Classify the response to the selected question\"\n",
    "            f\"into one of the following categories: {classes_names}\"\n",
    "            f\". \\n\\n ### Part of the interview ### \\nIntervier:\"\n",
    "            f\" {row['interview_question']} \\nResponse:\"\n",
    "            f\" {row['interview_answer']} \\n\\n### Selected Question ###\\n\"\n",
    "            f\"{row['question']} \\n\\nLabel:\"\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def predict(test, categories, model, tokenizer):\n",
    "\n",
    "    # Set logging level to ERROR to suppress INFO messages\n",
    "    logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "    y_pred = []\n",
    "    pipe = pipeline(task=\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    # max_new_tokens=4\n",
    "                    )\n",
    "\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "\n",
    "        # Determine the predicted category\n",
    "        for category in categories:\n",
    "            if category.lower() in answer.lower():\n",
    "                print(\"Right label:\" + answer.lower())\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            print(\"Wrong label:\" + answer.lower())\n",
    "            y_pred.append(\"none\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluation_report(y_true, y_pred, labels, run=None):\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    if run:\n",
    "        wandb_log_dict = {}\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    if run:\n",
    "        wandb_log_dict[\"Accuracy\"] = accuracy\n",
    "\n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped))\n",
    "                         if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.2f}')\n",
    "        if run:\n",
    "            wandb_log_dict[f\"Accuracy for label {labels[label]}\"] = label_accuracy\n",
    "\n",
    "    unsplit_labels = [label.replace(\" \", \"_\") for label in labels]\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped,\n",
    "                                         y_pred=y_pred_mapped,\n",
    "                                         target_names=unsplit_labels,\n",
    "                                         labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    report_columns = [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    report_table = []\n",
    "    class_report = class_report.splitlines()\n",
    "    for line in class_report[2:(len(labels)+2)]:\n",
    "        report_table.append(line.split())\n",
    "\n",
    "    # if run:\n",
    "    #     wandb_log_dict[\"Classification Report\"] = wandb.Table(\n",
    "    #         data=report_table,\n",
    "    #         columns=report_columns)\n",
    "\n",
    "    # For not predicted classes\n",
    "    mask = y_pred_mapped != -1\n",
    "    y_true_mapped2 = y_true_mapped[mask]\n",
    "    y_pred_mapped2 = y_pred_mapped[mask]\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped,\n",
    "                                   y_pred=y_pred_mapped,\n",
    "                                   labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # if run:\n",
    "    #     wandb_log_dict[\"Confusion Matix\"] = wandb.plot.confusion_matrix(\n",
    "    #         y_true=y_true_mapped2,\n",
    "    #         preds=y_pred_mapped2,\n",
    "    #         class_names=labels\n",
    "    #     )\n",
    "    #     run.log(wandb_log_dict)\n",
    "\n",
    "\n",
    "def evaluate(base_model_name, fine_tuned_model_path, label_name, run=None):\n",
    "\n",
    "    cache_dir = \"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        # torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        offload_folder=\"offload/\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "                                              cache_dir=cache_dir)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     # load_in_4bit=True,\n",
    "    #     quantization_config=BitsAndBytesConfig(\n",
    "    #         load_in_4bit=True,\n",
    "    #         bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    #     ),\n",
    "    #     device_map='auto',\n",
    "    #     # device_map='cpu',\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     cache_dir=cache_dir\n",
    "    # )\n",
    "\n",
    "    # base_model_reload.config.use_cache = False\n",
    "    # model = PeftModel.from_pretrained(base_model_reload,\n",
    "    #                                   fine_tuned_model_path,\n",
    "    #                                   # device_map='auto',\n",
    "    #                                   offload_folder=\"offload/\") \n",
    "    # model = model.merge_and_unload()\n",
    "\n",
    "    # Get test set data\n",
    "    test_df = pd.read_csv('preprocessed_data/test_set.csv')[[\n",
    "        'question',\n",
    "        'interview_question',\n",
    "        'interview_answer',\n",
    "        label_name\n",
    "    ]]\n",
    "\n",
    "    test_texts = create_test_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = list(test_df[label_name].unique())\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_true = test_df[label_name]\n",
    "    evaluation_report(y_true, y_pred, labels, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416479866c894819be0f539f5f496697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767249e2b8384f3b9a0191ba98b306f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f8102ef8474904aa2aee6f877f8a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fb36f3b2da43a3a24df663ddba4c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794983ad2dbe46d896e8accdc7f85691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c30b163594e8e9339584ccf1b2ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420764253a1046b7804ac2c6266f9aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f8fcdbc3cc4fa4a6813a298f1d1809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23b466c1d8c4f0db04b68abbfe0d892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815cf36208934a17905bc6e4c52eadba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad544876bdca44e68ff5f921db7e29bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f184bb0be3b419985d4a657a09bb8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6815744 || all params: 4547416064\n",
      "        || trainable%: 0.14988168894325302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of train test:You will be given a part of an interview. Classify the response to the selected question into one of the following categories: Explicit, General, Partial/half-answer, Dodging, Implicit, Deflection, Declining to answer, Claims ignorance, Clarification. \n",
      "\n",
      " ### Part of the interview ### \n",
      "Intervier: Q. Of the Biden administration. And accused the United States of containing China while pushing for diplomatic talks.How would you respond to that? And do you think President Xi is being sincere about getting the relationship back on track as he bans Apple in China? \n",
      "Response: Well, look, first of all, theI am sincere about getting the relationship right. And one of the things that is going on now is, China is beginning to change some of the rules of the game, in terms of trade and other issues.And so one of the things we talked about, for example, is that they're now talking about making sure that no Chineseno one in the Chinese Government can use a Western cell phone. Those kinds of things.And so, really, what this trip was aboutit was less about containing China. I don't want to contain China. I just want to make sure that we have a relationship with China that is on the up and up, squared away, everybody knows what it's all about. And one of the ways you do that is, you make sure that we are talking about the same things.And I think that one of the things we've doneI've tried to do, and I've talked with a number of my staff about this for the last, I guess, 6 monthsis, we have an opportunity to strengthen alliances around the world to maintain stability.That's what this trip was all about: having India cooperate much more with the United States, be closer with the United States, Vietnam being closer with the United States. It's not about containing China; it's about having a stable base, a stable base in the Indo-Pacific.And it'sfor example, when I was spending a lot of time talking with President Xi, he asked why we were doingwhy was I going to have the Quad, meaning Australia, India, Japan, and the United States? And I said, To maintain stability. It's not about isolating China. It's about making sure the rules of the roadeverything from airspace and space in the ocean isthe international rules of the road are abided by.And soand I hope thatI think that Prime Minister XiI mean, Xi has somesome difficulties right now. All countries end up with difficulties, and he had some economic difficulties he's working his way through. I want to see China succeed economically, but I want to see them succeed by the rules.The next question was to Bloomberg. \n",
      "\n",
      "### Selected Question ###\n",
      "Do you think President Xi is being sincere about getting the relationship back on track as he bans Apple in China? \n",
      "\n",
      "Label: General\n",
      "Example of validation test:You will be given a part of an interview. Classify the response to the selected question into one of the following categories: Dodging, Explicit, Claims ignorance, Declining to answer, Deflection, General, Partial/half-answer, Implicit, Clarification. \n",
      "\n",
      " ### Part of the interview ### \n",
      "Intervier: Q. ——becoming NATOization of Finland. Based on your long experience, how does that change Finland's place in the world? \n",
      "Response: Well, first of all, the context in which I said that was: The gentleman who occupies a seat on the other side of the—your border, in Moscow, said he wanted—I said he wanted the Findalization [Finlandization; White House correction] of NATO. I said it was more likely he's going to get the NATOization of Finland. [] That's what—that's the context in which that was said.And what was the second part of your question? \n",
      "\n",
      "### Selected Question ###\n",
      "How does the NATOization of Finland change Finland's place in the world? \n",
      "\n",
      "Label: Dodging\n",
      "Found 8 instances for training and\n",
      "    1 instances for validation.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.861300</td>\n",
       "      <td>2.799523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faac6665639c41768c1f4995af667c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     29\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m finetuning(base_model_name,\n\u001b[1;32m     30\u001b[0m                               fine_tuned_model_path,\n\u001b[1;32m     31\u001b[0m                               label_name,\n\u001b[1;32m     32\u001b[0m                               lr,\n\u001b[1;32m     33\u001b[0m                               epochs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#                  project=\"political-speech-clarity\",\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#                  id=\"8c0qfc9s\",\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#                  resume=\"must\")\u001b[39;00m\n\u001b[1;32m     40\u001b[0m evaluate(base_model_name,\n\u001b[1;32m     41\u001b[0m          fine_tuned_model_path,\n\u001b[1;32m     42\u001b[0m          label_name,\n\u001b[0;32m---> 43\u001b[0m          \u001b[43mrun\u001b[49m)\n\u001b[1;32m     45\u001b[0m run\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'lora_llm'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load the API key from the secret.json file\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    huggingface_hub.login(secrets.get('HF_KEY'))\n",
    "    # wandb.login(secrets.get('WANDB_KEY'))\n",
    "\n",
    "# 'TinyLlama/TinyLlama_v1.1'\n",
    "base_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "fine_tuned_model_path = \"./llama3.1\"\n",
    "label_name = \"evasion_label\"\n",
    "lr = 2e-4\n",
    "epochs = 1\n",
    "\n",
    "# Wandb configuration\n",
    "# run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "#                  project='political-speech-clarity',\n",
    "#                  job_type=\"training\",\n",
    "#                  # Track hyperparameters and run metadata\n",
    "#                  config={\n",
    "#                     \"learning_rate\": lr,\n",
    "#                     \"architecture\": base_model_name,\n",
    "#                     \"dataset\": \"qevasion_dataset_preproccessed\",\n",
    "#                     \"epochs\": epochs,\n",
    "#                  })\n",
    "\n",
    "model, tokenizer = finetuning(base_model_name,\n",
    "                              fine_tuned_model_path,\n",
    "                              label_name,\n",
    "                              lr,\n",
    "                              epochs)\n",
    "\n",
    "# run = wandb.init(entity=\"kontilenia-national-technical-university-of-athens\",\n",
    "#                  project=\"political-speech-clarity\",\n",
    "#                  id=\"8c0qfc9s\",\n",
    "#                  resume=\"must\")\n",
    "\n",
    "evaluate(base_model_name,\n",
    "         fine_tuned_model_path,\n",
    "         label_name)\n",
    "#          run)\n",
    "\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------\n",
    "                                    # Model evaluation before fine-tuning\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18033f7d6f354308ab331ecbc2a986fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3af970cbd2f401e8f0659623b9b1b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc3094dc6774428b1d02784fab39897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321ea23c182a4b15a8f7ee6a3787a07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e04bb15cc143cd934c22cb54d743ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51e6294d3d41bebbb05b33b12279c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29576a933a834fb7b63d168a45e74fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a51c96c8b412fb69d769b034bcd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ac656610154ab396510aaa0733c645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c607ca0470b74fd48f7e29c093568d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b767a8e7f0475fb1db3ba2ba73f475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0848675fd7754ddd9200df4fb61f40d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login(os.environ[\"hf_key\"])\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(base_model_name, fine_tuned_model_path, label_name, run=None):\n",
    "\n",
    "    cache_dir = \"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        offload_folder=\"offload/\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, \n",
    "                                              cache_dir=cache_dir)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     # load_in_4bit=True,\n",
    "    #     quantization_config=BitsAndBytesConfig(\n",
    "    #         load_in_4bit=True,\n",
    "    #         bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    #     ),\n",
    "    #     device_map='auto',\n",
    "    #     # device_map='cpu',\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     cache_dir=cache_dir\n",
    "    # )\n",
    "\n",
    "    # base_model_reload.config.use_cache = False\n",
    "    # model = PeftModel.from_pretrained(base_model_reload,\n",
    "    #                                   fine_tuned_model_path,\n",
    "    #                                   # device_map='auto',\n",
    "    #                                   offload_folder=\"offload/\") \n",
    "    # model = model.merge_and_unload()\n",
    "\n",
    "    # Get test set data\n",
    "    test_df = pd.read_csv('preprocessed_data/test_set.csv')[[\n",
    "        'question',\n",
    "        'interview_question',\n",
    "        'interview_answer',\n",
    "        label_name\n",
    "    ]]\n",
    "\n",
    "    test_texts = create_test_prompted_text(test_df, label_name)\n",
    "    dataset = pd.DataFrame(test_texts, columns=['text'])\n",
    "\n",
    "    labels = list(test_df[label_name].unique())\n",
    "\n",
    "    y_pred = predict(dataset, labels, model, tokenizer)\n",
    "    y_true = test_df[label_name]\n",
    "    evaluation_report(y_true, y_pred, labels, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  1%|          | 7/680 [00:27<35:04,  3.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 30/680 [01:44<31:25,  2.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      "\n",
      "### part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 38/680 [02:01<23:36,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 55/680 [02:43<22:36,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 70/680 [03:31<32:23,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 75/680 [03:42<24:38,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 77/680 [03:47<22:55,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 81/680 [03:54<16:37,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 87/680 [04:03<16:20,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 88/680 [04:04<14:50,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 99/680 [04:28<17:11,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 121/680 [05:18<17:52,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 126/680 [05:29<17:36,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 131/680 [05:38<16:47,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 169/680 [07:06<16:22,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 173/680 [07:17<19:50,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 179/680 [07:33<23:29,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 180/680 [07:35<20:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 184/680 [07:43<17:17,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 191/680 [08:00<17:22,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 210/680 [09:00<23:10,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 226/680 [09:33<12:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 238/680 [10:04<15:34,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 282/680 [11:28<09:30,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 365/680 [15:48<17:44,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 408/680 [19:53<16:10,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 417/680 [20:30<13:04,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 418/680 [20:32<10:39,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### reason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 430/680 [21:11<09:38,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 467/680 [23:34<10:31,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 469/680 [23:40<09:30,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 479/680 [24:17<11:22,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 534/680 [27:50<06:12,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 540/680 [28:05<05:23,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 580/680 [30:18<05:58,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:? \n",
      "\n",
      "### step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 628/680 [32:57<03:23,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      "\n",
      "### response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 673/680 [35:37<00:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong label:0\n",
      " response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [35:57<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "label_name = \"clarity_label\"\n",
    "\n",
    "# Get train set data\n",
    "df = pd.read_csv('preprocessed_data/train_set.csv')[['question',\n",
    "                                                     'interview_question',\n",
    "                                                     'interview_answer',\n",
    "                                                     label_name]]\n",
    "\n",
    "# Split train set to train and validation data\n",
    "np.random.seed(2024)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "validation = df[~msk]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_texts = create_prompted_text(train, label_name)\n",
    "validation_texts = create_prompted_text(validation,\n",
    "                                        label_name)\n",
    "\n",
    "# print(\"Example of train test:\" + train_texts[1])\n",
    "# print(\"Example of validation test:\" + validation_texts[1])\n",
    "\n",
    "\n",
    "def predict(test, model, tokenizer):\n",
    "\n",
    "    # Set logging level to ERROR to suppress INFO messages\n",
    "    logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "    y_pred = []\n",
    "    categories = list(df[label_name].unique())\n",
    "    pipe = pipeline(task=\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=4,\n",
    "                    temperature=0.1\n",
    "                    )\n",
    "\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "\n",
    "        # Determine the predicted category\n",
    "        for category in categories:\n",
    "            if category.lower() in answer.lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            print(\"Wrong label:\" + answer.lower())\n",
    "            y_pred.append(\"none\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "validation_texts = create_test_prompted_text(validation, label_name)\n",
    "\n",
    "dataset = pd.DataFrame(validation_texts, columns=['text'])\n",
    "y_pred = predict(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.341\n",
      "Accuracy for label Direct Reply: 0.853\n",
      "Accuracy for label Indirect: 0.013\n",
      "Accuracy for label Direct Non-Reply: 0.562\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Direct Reply       0.38      0.85      0.53       218\n",
      "        Indirect       0.33      0.01      0.02       389\n",
      "Direct Non-Reply       0.30      0.56      0.39        73\n",
      "\n",
      "       micro avg       0.36      0.34      0.35       680\n",
      "       macro avg       0.34      0.48      0.31       680\n",
      "    weighted avg       0.34      0.34      0.22       680\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[186   2  20]\n",
      " [284   5  77]\n",
      " [ 20   8  41]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = list(df[label_name].unique())\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "y_true = validation[label_name]\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'none',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Indirect',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'none',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Non-Reply',\n",
       " 'Direct Reply',\n",
       " 'Direct Reply']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classify the interviewee's response into one of the following\\n        categories: Indirect, Direct Reply, Direct Non-Reply. Analyze the part of the interview provided below, focusing\\n        specifically on the interviewee's response to the marked question.\\n\\n ### Part of the interview ### \\n Intervier:Q. Hi, I'm Jongjin Park of Money Today. First of all, I would like to ask a question to President Yoon Suk Yeol of the Republic of Korea. I heard that you stated that a new chapter has opened in our trilateral cooperation with the two countries. Compared to the previous summits, what would be the most significant outcome that you gained through this summit?And also, from the perspective of our people, what would be the benefit that the people of Korea would feel from these strengthening of ties?And now my question goes to President Biden. During this summit, the issues of detainees or prisoners of wars—and you mentioned that there will be further cooperation in these human rights issues. And you also said you will support the free and peaceful Korean Peninsula in the region. And what would—what kind of shift would there be in your policy? And what kind of specific solutions do you have in this regard?Lastly, I would like to direct my question to Prime Minister Kishida. Today we had a historic trilateral summit. However, there was much backlash and many concerns in Korea. However, President Yoon showed his political courage to do so. That's the international community's evaluation.However, there are still concerns that the—Japan is making very passive efforts to resolve our issues that still remain. And also, how would you be able to show your truthful willingness to resolve and improve our bilateral relations going forward? \\n Response: First of all, this trilateral cooperation amongst our three countries has opened a new chapter, and we made that announcement today to talk about the differences from the past cooperation.For instance, in the past, it was about individual issues that we sought cooperation among ourselves. But now, as we have opened a new chapter in our cooperation for security, economy, science and technology, and development cooperation for the Global South, health, and women—across all of these issues, our three countries decided to closely work together. So it's much more comprehensive in nature.Such comprehensive cooperation has been launched by us today because currently we face complicated crisis and the threat from the D.P.R.K. And across the world, we believe that we can together make a contribution to freedom and peace around the world.So that is our foundational understanding and our common and shared interests of the three countries. And not just for exclusionary interests of ourselves. Our interests are well aligned with the universal interests of the members of the global community. That's where we find our shared interests lie.And at the same time, this framework of comprehensive cooperation among our three countries will contribute to global supply chain resilience, global financial market stability, cooperation in the frontier technology sectors and science.Our three countries together have the best-in-class expertise in science and technology. And we are the ones who are implementing liberal democracies. Naturally, progress in science and technologies will bring benefits—tangible benefits—to our people, not just in terms of security, but also in terms of economy and science and technology.But what is most important here is not about our own interests only. When we put our forces together, I believe that we can make a contribution to the advancement of freedom and peace in the world. And that's exactly where our interests are aligned. I—look, back in May of 2022, I met with the families of the Japanese abductees during my visit, heard their stories, and empathized with them and got a sense of the pain they're feeling. It's real.We know there are many families out there who still wait and worry and wonder. And we're not going to forget about them or their loved ones.And there's clear language on this on our joint statement. The bottom line is this: That we share a common position; we're committed to working together to see the return of all prisoners of war and those who've been abducted and detained.And by the way, one of the things we get asked many times—and it wasn't directly asked, but implied—is what makes us think any of this is positive. Success brings success. When other nations see cooperation in the region, they make judgments about: Would they be better off if they made commitments? Will they move?Think about—as students of history, all of you; and you are—think about how many times successes have generated other successes when you don't anticipate it.And so I just think this is a—we're not going to forget, we're not giving up, and we're going to continue to make the case for the freedom of all of those detainees. With regard to your question for me, first of all, I have strong feelings about strengthening bilateral relationships between the R.O.K. and Japan. I share that with The two countries, in dealing with international challenges, should cooperate. We're both important neighboring countries. And so friendship with President Yoon and a relationship of trust, based on this, both countries as partners should open up a new era. And that is my thinking.This year, President Yoon came to Japan, and I visited the R.O.K. At international fora, we have repeated meetings—we have had repeated meetings. And between our two countries, including the economy and security, we've had forward-looking and concrete approaches which were started. It's already in motion, dynamically.Economic security dialogue was started—or it has been decided on. In the area of export control, there have been progress. And also Financial Ministers and Defense Ministers have had meetings.And so we've had this very positive, forward-looking developments. And these are seen not only in the public sector. Also in the private sector, we see a slate of developments, human exchanges, and exchanges between business circles. We're seeing very active developments in all of these areas, and that is a reality.Going forward, we hope to accumulate these approaches with—along with President Yoon to strengthen our bilateral relations even further. By generating results, we hope that people will understand Japan's feelings towards our bilateral relations. And we'd like to continue such efforts.Thank you. Thank you very much.Then, let me see. From Kyodo—Tajiri-san, Kyodo News. \\n\\n### Question ###\\n\\nFrom the perspective of the people of Korea, what would be the benefit felt from the strengthening of ties?\\n\\n Label: \""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.375\n",
      "Accuracy for label Direct Reply: 0.853\n",
      "Accuracy for label Indirect: 0.072\n",
      "Accuracy for label Direct Non-Reply: 0.562\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Direct Reply       0.38      0.85      0.53       218\n",
      "        Indirect       0.54      0.07      0.13       389\n",
      "Direct Non-Reply       0.30      0.56      0.39        73\n",
      "\n",
      "        accuracy                           0.38       680\n",
      "       macro avg       0.41      0.50      0.35       680\n",
      "    weighted avg       0.46      0.38      0.28       680\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[186  12  20]\n",
      " [284  28  77]\n",
      " [ 20  12  41]]\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = [label.replace(\"none\", \"Indirect\") for label in y_pred]\n",
    "y_pred1\n",
    "evaluate(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5n1KZofVGTZFVLtQb7p3Q",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
