{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft transformers"
      ],
      "metadata": {
        "id": "FwWDe8KOhbo8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GxdGxj4zpThB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train dataset\n",
        "ds = load_dataset(\"ailsntua/QEvasion\")\n",
        "\n",
        "# Convert to pandas and keep only useful columns\n",
        "df_train = ds[\"train\"].to_pandas()[[\"question\",\"interview_answer\",\n",
        "                                   \"label\",\"annotator_id\"]]\n",
        "\n",
        "# Create dictionary with key as the annotator and value the Dataframe with\n",
        "# only the corresponding sQAs\n",
        "split_dfs = {\n",
        "    category: group[[\"question\",\"interview_answer\", \"label\"]]\n",
        "    for category, group in df_train.groupby('annotator_id')\n",
        "    }"
      ],
      "metadata": {
        "id": "0KmafoeQhRhW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_train_set = split_dfs[85]\n",
        "temp_train_set"
      ],
      "metadata": {
        "id": "8t1S39Dprstk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Load pre-trained Roberta model and tokenizer\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,     # Task type: sequence classification\n",
        "    r=8,                            # Rank of the low-rank adaptation\n",
        "    lora_alpha=16,                  # Scaling factor\n",
        "    lora_dropout=0.1,               # Dropout probability for LoRA layers\n",
        "    target_modules=[\"query\", \"value\"] # Target modules to inject LoRA into (attention projections)\n",
        ")\n",
        "\n",
        "# Add LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Freeze the original model parameters (LoRA will only train the adapters)\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define the dataset and data loader\n",
        "# Assuming the dataset returns a dictionary with 'input_ids', 'attention_mask', and 'labels'\n",
        "dataset = ...  # Your dataset here\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Move model to the appropriate device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # adjust the number of epochs as needed\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "# Save the model with LoRA adapters\n",
        "model.save_pretrained(\"roberta-lora\")"
      ],
      "metadata": {
        "id": "pvDjPalbtmuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}