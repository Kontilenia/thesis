{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def pattern_cleaning(\n",
    "    df: pd.DataFrame,\n",
    "    exceptions: List[int]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that cleans 4 unwanted patterns from the dataset\n",
    "    regarding, indexing of questions, special characters, speaker's name\n",
    "    and description of questions.\n",
    "\n",
    "    Arguments:\n",
    "    df – Dataframe to be cleaned\n",
    "    exceptions - exception list of indexes where the disception of the\n",
    "    question is needed\n",
    "\n",
    "    Returns:\n",
    "    df – Cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Regex explanation:\n",
    "\n",
    "    ^ matches the start of the string\n",
    "    (\\d+\\.|Part \\d+:|Q\\d*:|\\d+\\. Q\\d*: ) is a capturing group that\n",
    "    matches one of the following:\n",
    "        \\d+\\. : one or more digits followed by a period\n",
    "\n",
    "        Part \\d+: : the string \"Part \" followed by one or more digits,\n",
    "        a colon, and an optional space\n",
    "\n",
    "        Q\\d*: : the string \"Q\" followed by one or more digits, a colon,\n",
    "        and an optional space\n",
    "\n",
    "        \\d+\\. Q\\d*: : one or more digits followed by a period, a space,\n",
    "        \"Q\", one or more digits, a colon, and an optional space\n",
    "\n",
    "        - : start sentence with \"-\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Remove indexing from questions\n",
    "    index_pattern = r'^(\\d+\\. Q\\d+:|\\d+\\.|Part \\d+:|Q\\d+:|-)'\n",
    "    df['question'] = df['question'].str.replace(\n",
    "        index_pattern,\n",
    "        '',\n",
    "        regex=True\n",
    "        )\n",
    "\n",
    "    # 2) Remove quotes and new line espace characters\n",
    "    df['question'] = df['question'].str.replace(\n",
    "        r'[\"\\n]',\n",
    "        '',\n",
    "        regex=True\n",
    "        )\n",
    "    df['interview_answer'] = df['interview_answer'].str.replace(\n",
    "        r'\\n',\n",
    "        '',\n",
    "        regex=True\n",
    "        )\n",
    "\n",
    "    # 3) Remove first sentence from answer (indicates which present is\n",
    "    # speaking)\n",
    "    sentence_pattern = r'^[^.]+\\.?'\n",
    "    df['interview_answer'] = df['interview_answer'].str.replace(\n",
    "        sentence_pattern,\n",
    "        '',\n",
    "        regex=True\n",
    "        )\n",
    "\n",
    "    # 4) Remove description from questions\n",
    "    df.loc[~df.index.isin(exceptions), 'question'] = df.loc[\n",
    "        ~df.index.isin(exceptions), 'question'].apply(\n",
    "        lambda x: re.sub(r'^[^:]+: ', '', x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_italic_sentences(url: str) -> list:\n",
    "    \"\"\"\n",
    "    Function to get italic sentences from a url, optimized with error\n",
    "    handling\n",
    "\n",
    "    Arguments:\n",
    "    url - Link of the text\n",
    "\n",
    "    Returns:\n",
    "    Text with italics except specific phrases\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract text from the <div> with class \"field-docs-content\"\n",
    "        div_content = soup.find('div', class_='field-docs-content')\n",
    "\n",
    "        # Return an empty list if the div is not found\n",
    "        if div_content is None:\n",
    "            return []\n",
    "\n",
    "        exception_list = {\n",
    "            \"The President.\",\n",
    "            \"Q.\",\n",
    "            \"Inaudible\",\n",
    "            \"inaudible\"\n",
    "            }\n",
    "\n",
    "        # Extract unique sentences from <i> or <em> tags, excluding\n",
    "        # specific phrases\n",
    "        italic_sentences = {\n",
    "            i.get_text(strip=True)\n",
    "            for i in div_content.find_all(['i', 'em'])\n",
    "            }\n",
    "        return [\n",
    "            sentence\n",
    "            for sentence in italic_sentences\n",
    "            if sentence not in exception_list\n",
    "            ]\n",
    "\n",
    "    except (requests.RequestException, AttributeError) as e:\n",
    "        print(f\"Error retrieving or parsing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_interview_answer(row: pd.Series, url_sentences: set) -> str:\n",
    "    \"\"\"\n",
    "    Remove unnecessary sentences from a interview_answer in a\n",
    "    vectorized manner\n",
    "\n",
    "    Arguments:\n",
    "    row: row of a dataframe\n",
    "    url_sentences: set of unique sentences to be removed\n",
    "    from interview answer of a text coming from a particular\n",
    "    url\n",
    "\n",
    "    Returns:\n",
    "    Interview answer string with removed sentences\n",
    "    \"\"\"\n",
    "    unique_sentences = url_sentences.get(row['url'], [])\n",
    "    interview_answer = row['interview_answer']\n",
    "    for sentence in unique_sentences:\n",
    "        interview_answer = interview_answer.replace(sentence, '')\n",
    "    return interview_answer\n",
    "\n",
    "\n",
    "def remove_unrelated_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to remove italic sentences from the 'interview_answer' column.\n",
    "\n",
    "    Arguments:\n",
    "    df – Dataframe to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    df – Cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary to store unique sentences for each URL\n",
    "    url_sentences = {}\n",
    "\n",
    "    # Create a dictionary to store unique sentences for each URL\n",
    "    unique_urls = df['url'].unique()\n",
    "\n",
    "    # Get sentences for each URL (optionally use parallel processing for\n",
    "    # speedup)\n",
    "    for url in unique_urls:\n",
    "        url_sentences[url] = get_italic_sentences(url)\n",
    "\n",
    "    df['interview_answer'] = df.apply(\n",
    "        lambda x: clean_interview_answer(x, url_sentences), axis=1)\n",
    "\n",
    "    # Optional: Clean up whitespace after sentence removal\n",
    "    df['interview_answer'] = df['interview_answer'].str.replace(\n",
    "        r'\\s+', ' ',\n",
    "        regex=True\n",
    "        ).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "def extra_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Add inadible and multiple question labels to the dataset\n",
    "\n",
    "  Arguments:\n",
    "  df – Dataframe\n",
    "\n",
    "  Returns:\n",
    "  df – Labeled dataframe\n",
    "  \"\"\"\n",
    "  df[\"inaudible\"] = df['interview_answer'].str.contains('inaudible', case=False)\n",
    "  df[\"multiple_questions\"] = df['question'].str.count('\\?') > 1\n",
    "  df[\"affirmative_questions\"] = ~df['question'].str.contains('\\?')\n",
    "  return df\n",
    "\n",
    "def main():\n",
    "  # Load train dataset\n",
    "  ds = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "  # Convert to pandas and keep only useful columns\n",
    "#   df_train = ds[\"train\"].to_pandas()[[\"question\",\"interview_question\",\n",
    "#                                     \"interview_answer\", \"label\",\"url\"]]\n",
    "  \n",
    "  df_train = ds[\"train\"].to_pandas()\n",
    "\n",
    "  # Remove unwanted patterns\n",
    "  exception_list = [142,493,699,809,1052,1053,1446,\n",
    "                    2417,2631,2821,3181,3390]\n",
    "  df_train = pattern_cleaning(df_train, exception_list)\n",
    "\n",
    "  # Extract noise from the end of interview answer\n",
    "  df_train = remove_unrelated_text(df_train)\n",
    "\n",
    "  # Add 2 more labels for multiple questions and inadible speech\n",
    "  df_train = extra_labels(df_train)\n",
    "\n",
    "  # df_train.to_csv('preprocessed_data/train_set.csv', index=False)\t\n",
    "  df_train.to_csv('preprocessed_data/full_train_set.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
